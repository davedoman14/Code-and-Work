{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from statistics import mean\n",
    "\n",
    "from nltk.util import ngrams\n",
    "import nltk.collocations as co_loc\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import nltk\n",
    "from nltk import FreqDist, word_tokenize\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder, TrigramAssocMeasures, TrigramCollocationFinder\n",
    "from nltk.corpus import PlaintextCorpusReader, stopwords\n",
    "from nltk.text import Text \n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPAMDIR = 'corpus/spam'\n",
    "HAMDIR = 'corpus/ham'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AllFiles(MainDirec):\n",
    "   for dirpath,_,corpus in os.walk(MainDirec):\n",
    "       for find in corpus:\n",
    "           yield os.path.abspath(os.path.join(dirpath, find))\n",
    "\n",
    "def processspamham(MainDirec, Label):    \n",
    "    BothTexts = [] \n",
    "    FileDirectory = AllFiles(MainDirec)\n",
    "    AllEmails = []\n",
    "    for find in FileDirectory:\n",
    "        AllEmails.append(find)\n",
    "    # process all files\n",
    "    for find in AllEmails:\n",
    "        if (find.endswith(\".txt\")):\n",
    "                       \n",
    "            with open(find, 'r', encoding = 'latin-1') as final:\n",
    "                BothTexts.append(final.read())\n",
    "    emaildocs = [] \n",
    "    # process each email\n",
    "    for Text in BothTexts:\n",
    "        Tokens = nltk.word_tokenize(Text)\n",
    "        emaildocs.append((Tokens, Label))\n",
    "    return BothTexts, emaildocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractTokens(emaildocs):\n",
    "    Tokens = []\n",
    "    for doc in emaildocs:\n",
    "        for w in doc[0]:\n",
    "            Tokens.append(w)\n",
    "    return Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextsSpam, SPAM = processspamham(SPAMDIR, 'spam')\n",
    "TextsHam, HAM = processspamham(HAMDIR, 'ham')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Spam Emails: 1500\n",
      "Total Ham Emails: 3672\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Spam Emails:\",len(SPAM))\n",
    "print(\"Total Ham Emails:\",len(HAM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Emails: 5172\n"
     ]
    }
   ],
   "source": [
    "#Combine SPAM and HAM to get Email Total\n",
    "emaildocs = SPAM + HAM\n",
    "print(\"Total Emails:\",len(emaildocs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355375\n",
      "['Subject', ':', 'dobmeos', 'with', 'hgh', 'my', 'energy', 'level', 'has', 'gone', 'up', '!', 'stukm', 'introducing', 'doctor', '-', 'formulated', 'hgh', 'human', 'growth', 'hormone', '-', 'also', 'called', 'hgh', 'is', 'referred', 'to', 'in', 'medical', 'science', 'as', 'the', 'master', 'hormone', '.', 'it', 'is', 'very', 'plentiful', 'when', 'we', 'are', 'young', ',', 'but', 'near', 'the', 'age', 'of', 'twenty', '-', 'one', 'our', 'bodies', 'begin', 'to', 'produce', 'less', 'of', 'it', '.', 'by', 'the', 'time', 'we', 'are', 'forty', 'nearly', 'everyone', 'is', 'deficient', 'in', 'hgh', ',', 'and', 'at', 'eighty', 'our', 'production', 'has', 'normally', 'diminished', 'at', 'least', '90', '-', '95', '%', '.', 'advantages', 'of', 'hgh', ':', '-', 'increased', 'muscle', 'strength', '-', 'loss', 'in', 'body', 'fat', '-', 'increased', 'bone', 'density', '-', 'lower', 'blood', 'pressure', '-', 'quickens', 'wound', 'healing', '-', 'reduces', 'cellulite', '-', 'improved', 'vision', '-', 'wrinkle', 'disappearance', '-', 'increased', 'skin', 'thickness', 'texture', '-', 'increased', 'energy', 'levels', '-', 'improved', 'sleep', 'and', 'emotional', 'stability', '-', 'improved', 'memory', 'and', 'mental', 'alertness', '-', 'increased', 'sexual', 'potency', '-', 'resistance', 'to', 'common', 'illness', '-', 'strengthened', 'heart', 'muscle', '-', 'controlled', 'cholesterol', '-', 'controlled', 'mood', 'swings', '-', 'new', 'hair', 'growth', 'and', 'color', 'restore', 'read', 'more', 'at', 'this', 'website', 'unsubscribe', 'Subject', ':', 'your', 'prescription', 'is', 'ready', '.', '.', 'oxwq', 's', 'f', 'e', 'low', 'cost', 'prescription', 'medications', 'soma', ',', 'ultram', ',', 'adipex', ',', 'vicodin', 'many', 'more', 'prescribed', 'online', 'and', 'shipped', 'overnight', 'to', 'your', 'door', '!', '!', 'one', 'of', 'our', 'us', 'licensed', 'physicians', 'will', 'write', 'an', 'fda', 'approved', 'prescription', 'for', 'you', 'and', 'ship', 'your', 'order', 'overnight', 'via', 'a', 'us', 'licensed', 'pharmacy', 'direct', 'to', 'your', 'doorstep', '.', '.', '.', '.', 'fast', 'and', 'secure', '!', '!', 'click', 'here', '!', 'no', 'thanks', ',', 'please', 'take', 'me', 'off', 'your', 'list', 'ogrg', 'z', 'lqlokeolnq', 'lnu', 'Subject', ':', 'get', 'that', 'new', 'car', '8434', 'people', 'nowthe', 'weather', 'or', 'climate', 'in', 'any', 'particular', 'environment', 'can', 'change', 'and', 'affect', 'what', 'people', 'eat', 'and', 'how', 'much', 'of', 'it', 'they', 'are', 'able', 'to', 'eat', '.', 'Subject', ':', 'await', 'your', 'response', 'dear', 'partner', ',', 'we', 'are', 'a', 'team', 'of', 'government', 'officials', 'that', 'belong', 'to', 'an', 'eight', '-', 'man', 'committee', 'in', 'the', 'presidential', 'cabinet', 'as', 'well', 'as', 'the', 'senate', '.', 'at', 'the', 'moment', ',', 'we', 'will', 'be', 'requiring', 'your', 'assistance', 'in', 'a', 'matter', 'that', 'involves', 'investment', 'of', 'monies', ',', 'which', 'we', 'intend', 'to', 'transfer', 'to', 'your', 'account', ',', 'upon', 'clarification', 'and', 'a', 'workable', 'agreement', 'reached', 'in', 'consummating', 'the', 'project', 'with', 'you', '.', 'based', 'on', 'a', 'recommendation', 'from', 'an', 'associate', 'concerning', 'your', 'integrity', ',', 'loyalty', 'and', 'understanding', ',', 'we', 'deemed', 'it', 'necessary', 'to', 'contact', 'you', 'accordingly', '.', 'all', 'arrangements', 'in', 'relation', 'to', 'this', 'investment', 'initiative', ',', 'as', 'well', 'as', 'the', 'initial', 'capital', 'for', 'its', 'take', 'off', 'has', 'been', 'tactically', 'set', 'aside', 'to', 'commence', 'whatever', 'business', 'you', 'deemed', 'fit', ',', 'that', 'will', 'turn', 'around', 'profit', 'favourably', '.', 'we', 'request', 'you', 'immediately', 'contact', 'us', 'if', 'you', 'will', 'be', 'favorably', 'disposed', 'to', 'act', 'as', 'a', 'partner', 'in', 'this', 'venture', ',', 'and', 'possibly', 'will', 'afford', 'us', 'the', 'opportunity', 'to', 'discuss', 'whatever', 'proposal', 'you', 'may', 'come', 'up', 'with', '.', 'also', 'bear', 'in', 'mind', 'that', 'the', 'initial', 'capital', 'that', 'we', 'shall', 'send', 'across', 'will', 'not', 'exceed', '$', '13', ',', '731', ',', '000', ',', '00', 'usd', '(', 'thirteen', 'million', 'seven', 'hundred', 'and', 'thirty', 'one', 'thousand', 'united', 'states', 'dollars', ')', 'so', 'whatever', 'areas', 'of', 'investment', 'your', 'proposal', 'shall', 'cover', ',', 'please', 'it', 'should', 'be', 'within', 'the', 'set', 'aside', 'capital', '.', 'in', 'this', 'regard', ',', 'the', 'proposal', 'you', 'may', 'wish', 'to', 'discuss', 'with', 'us', 'should', 'be', 'comprehensive', 'enough', 'for', 'our', 'better', 'understanding', ';', 'with', 'special', 'emphasis', 'on', 'the', 'following', ':', '1', '.', 'the', 'tax', 'obligationin', 'your', 'country', '2', '.', 'the', 'initial', 'capital', 'base', 'required', 'in', 'your', 'proposed', 'investment', 'area', ',', 'as', 'well', 'as', ';', '3', '.', 'the', 'legal', 'technicalities', 'in', 'setting', 'up', 'a', 'business', 'in', 'your', 'country', 'with', 'foreigners', 'as', 'share', '-', 'holders', '4', '.', 'the', 'most', 'convenient', 'and', 'secured', 'mode', 'of', 'receiving', 'the', 'funds', 'without', 'our', 'direct', 'involvement', '.', '5', '.', 'your', 'ability', 'to', 'provide', 'a', 'beneficiary', '/', 'partnership', 'account', 'with', 'a', 'minimal', 'deposit', ',', 'where', 'we', 'shall', 'transfer', 'the', 'funds', 'into', 'subsequently', '.', 'another', 'area', 'that', 'we', 'wish', 'to', 'explicitly', 'throw', 'more', 'light', 'on', ',', 'is', 'the', 'process', 'we', 'have', 'conceived', 'in', 'transferring', 'the', 'funds', 'into', 'the', 'account', 'you', 'shall', 'be', 'providing', '.', 'since', 'we', 'are', 'the', 'owners', 'of', 'the', 'funds', ',', 'and', 'the', 'money', 'will', 'be', 'leaving', 'the', 'apex', 'bank', 'of', 'my', 'country', ',', 'we', 'shall', 'purposefully', 'fulfill', 'the', 'legal', 'obligations', 'precedent', 'to', 'transferring', 'such', 'huge', 'amount', 'of', 'funds', ',', 'without', 'arousing', 'suspicion', 'from', 'any', 'quarter', 'as', 'a', 'drug', 'or', 'terrorist', 'related', 'funds', ';', 'and', 'this', 'will', 'assist', 'us', 'in', 'the', 'long', 'run', 'to', 'forestall', 'any', 'form', 'of', 'investigations', '.', 'remember', 'that', ',', 'on', 'no', 'account', 'must', 'we', 'be', 'seen', 'or', 'perceived', 'to', 'be', 'directly', 'connected', 'with', 'the', 'transfer', 'of', 'funds', '.', 'you', 'will', 'be', 'the', 'one', 'to', 'be', 'doing', 'all', 'these', ',', 'and', 'in', 'the', 'course', 'of', 'transfer', ',', 'if', 'for', 'any', 'reason', 'whatsoever', ',', 'you', 'incurred', 'some', 'bills', ',', 'we', 'shall', 'adequately', 'retire', 'same', ',', 'upon', 'the', 'successful', 'confirmation', 'of', 'the', 'funds', 'in', 'your', 'account', '.', 'the', 'commencement', 'of', 'this', 'project', 'is', 'based', 'on', 'your', 'ability', 'to', 'convince', 'us', 'of', 'the', 'need', 'to', 'invest', 'in', 'whatever', 'business', 'you', 'have', 'chosen', ',', 'and', 'to', 'trust', 'your', 'personality', 'and', 'status', ',', 'especially', 'as', 'it', 'concerns', 'the', 'security', 'of', 'the', 'funds', 'in', 'your', 'custody', '.', 'i', 'await', 'your', 'response', ',', 'sincerely', ',', 'john', 'adams', '(', 'chairman', 'senate', 'committee', 'on', 'banks', 'and', 'currency', ')', 'call', 'number', ':', '234', '-', '802', '-', '306', '-', '8507', 'Subject', ':', 'coca', 'cola', ',', 'mbna', 'america', ',', 'nascar', 'partner', 'with', 'otcbb', ':', 'imts', 'stock', 'profile', 'about', 'company', 'investment', 'highlights', 'press', 'release', '12', '/', '01', '/', '2003', 'indianapolis', ',', 'in', '-', 'race', 'car', 'simulators', '?', 'inks', 'the', 'sale', 'of', 'eight', 'simulators', 'for', 'installation', 'in', 'moscow', '09', '/', '17', '/', '2003', 'indianapolis', ',', 'in', '-', 'nascar', 'silicon', 'motor', 'speedway', '?', 'simulators', 'go', 'international', '09', '/', '05', '/', '2003', 'indianapolis', ',', 'in', '-', 'nascar', 'silicon', 'motor', 'speedway', '?', 'expands', 'to', 'monterey', ',', 'california', \"'\", 's', 'famed', 'cannery', 'row', '09', '/', '02', '/', '2003', 'indianapolis', ',', 'in', '-', 'nascar', 'silicon', 'motor', 'speedway', '?', 'announces', 'custom', 'upgrades', 'to', 'world', \"'\", 's', 'most', 'realistic', 'racing', 'simulation', '08', '/', '14', '/', '2003', 'indianapolis', ',', 'in', '-', 'race', 'car', 'simulators', '?', 'and', 'baldacci', 'sign', 'agreement', 'to', 'develop', 'international', 'markets', 'for', 'the', 'new', 'generation', 'race', 'simulutors', '08', '/', '12', '/', '2003', 'indianapolis', ',', 'in', '-', 'imts', 'forms', 'new', 'subsidiary', 'for', 'manufacturing', 'and', 'sales', 'of', 'race', 'car', 'simulators', '08', '/', '07', '/', '2003', 'indianapolis', ',', 'in', '-', 'nascar', 'silicon', 'motor', 'speedway', '?', 'renews', 'licensing', 'agreement', 'with', 'speedway', 'motorsports', ',', 'inc', '.', ',', 'for', 'race', 'track', 'simulators', '08', '/', '05', '/', '2003', 'indianapolis', ',', 'in', '-', 'nascar', 'silicon', 'motor', 'speedway', '?', ',', 'int', '.', 'speedway', 'corp', '.', 'renew', 'licensing', 'agreement', 'for', 'race', 'track', 'simulators', '07', '/', '27', '/', '2003', 'indianapolis', ',', 'in', '-', 'nascar', 'silicon', 'motor', 'speedway', '?', 'simulators', 'to', 'be', 'installed', 'at', 'st', '.', 'louis', 'nascar', 'speedpark', 'location', '07', '/', '24', '/', '2003', 'indianapolis', ',', 'in', '-', 'nascar', 'silicon', 'motor', 'speedway', '?', 'operator', 'gets', 'exclusive', 'five', '-', 'year', 'nascar', 'license', 'extension', '05', '/', '30', '/', '2003', 'nashville', ',', 'tn', '-', 'nascar', 'silicon', 'motor', 'speedway', '?', 'at', 'opry', 'mills', 'to', 'host', 'official', 'media', 'luncheon', 'for', 'nashville', 'superspeedway', \"'\", 's', 'trace', 'adkins', 'chrome', '300', 'event', '04', '/', '22', '/', '2003', 'indianapolis', ',', 'in', '-', 'nascar', 'silicon', 'motor', 'speedway', '?', 'simulators', 'now', 'running', 'at', 'nascar', 'speedpark', '03', '/', '19', '/', '2003', 'indianapolis', ',', 'in', '-', 'nascar', 'silicon', 'motor', 'speedway', '?', 'expansion', 'plans', 'begin', 'at', 'two', 'burroughs', 'chapin', 'entertainment', 'venues', '02', '/', '27', '/', '2003', 'indianapolis', ',', 'in', '-', 'nascar', 'silicon', 'motor', 'speedway', '?', 'to', 'determine', 'national', 'champion', 'among', 'simulator', 'racers', '02', '/', '14', '/', '2003', 'indianapolis', ',', 'in', '-', 'partnerships', 'with', 'coca', '-', 'cola', ',', 'mbna', 'and', 'in', 'demand', 'boost', 'nascar', 'silicon', 'motor', 'speedway', '?', 'racing', 'centers', '02', '/', '28', '/', '2003', 'indianapolis', ',', 'in', '-', 'nascar', 'drivers', 'sadler', ',', 'nadeau', 'give', 'thumbs', 'up', 'to', 'indianapolis', 'simulation', 'at', 'nascar', 'silicon', 'motor', 'speedway', '?', '02', '/', '22', '/', '2003', 'indianapolis', ',', 'in', '-', 'star', 'studded', 'lineup', 'for', 'make', 'a', 'wish', 'fundraiser', 'at', 'nashville', 'nascar', 'silicon', 'motor', 'speedway', 'location', '01', '/', '14', '/', '2003', 'indianapolis', ',', 'in', '-', 'indianapolis', 'motor', 'speedway', 'to', 'be', 'added', 'to', 'nascar', 'silicon', 'motor', 'speedway', 'simulators', '*', '*', '*', '*', '*', '*', '*', 'important', 'notice', 'and', 'disclaimer', ':', 'please', 'read', '*', '*', '*', '*', '*', '*', '*', 'intelligent', 'stock', 'picks', ',', 'and', 'affiliates', '(', 'isp', ')', ',', 'publishes', 'reports', 'providing', 'information', 'on', 'selected', 'companies', 'that', 'isp', 'believes', 'has', 'investment', 'potential', '.', 'isp', 'is', 'not', 'a', 'registered', 'investment', 'advisor', 'or', 'broker', '-', 'dealer']\n"
     ]
    }
   ],
   "source": [
    "#Original amount of SPAM tokens without any cleaning\n",
    "print(len(ExtractTokens(SPAM)))\n",
    "print(ExtractTokens(SPAM)[:1400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "830750\n",
      "['Subject', ':', 'christmas', 'tree', 'farm', 'pictures', 'Subject', ':', 'vastar', 'resources', ',', 'inc', '.', 'gary', ',', 'production', 'from', 'the', 'high', 'island', 'larger', 'block', 'a', '-', '1', '#', '2', 'commenced', 'on', 'saturday', 'at', '2', ':', '00', 'p', '.', 'm', '.', 'at', 'about', '6', ',', '500', 'gross', '.', 'carlos', 'expects', 'between', '9', ',', '500', 'and', '10', ',', '000', 'gross', 'for', 'tomorrow', '.', 'vastar', 'owns', '68', '%', 'of', 'the', 'gross', 'production', '.', 'george', 'x', '3', '-', '6992', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'forwarded', 'by', 'george', 'weissman', '/', 'hou', '/', 'ect', 'on', '12', '/', '13', '/', '99', '10', ':', '16', 'am', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'daren', 'j', 'farmer', '12', '/', '10', '/', '99', '10', ':', '38', 'am', 'to', ':', 'carlos', 'j', 'rodriguez', '/', 'hou', '/', 'ect', '@', 'ect', 'cc', ':', 'george', 'weissman', '/', 'hou', '/', 'ect', '@', 'ect', ',', 'melissa', 'graves', '/', 'hou', '/', 'ect', '@', 'ect', 'subject', ':', 'vastar', 'resources', ',', 'inc', '.', 'carlos', ',', 'please', 'call', 'linda', 'and', 'get', 'everything', 'set', 'up', '.', 'i', \"'\", 'm', 'going', 'to', 'estimate', '4', ',', '500', 'coming', 'up', 'tomorrow', ',', 'with', 'a', '2', ',', '000', 'increase', 'each', 'following', 'day', 'based', 'on', 'my', 'conversations', 'with', 'bill', 'fischer', 'at', 'bmar', '.', 'd', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'forwarded', 'by', 'daren', 'j', 'farmer', '/', 'hou', '/', 'ect', 'on', '12', '/', '10', '/', '99', '10', ':', '34', 'am', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'enron', 'north', 'america', 'corp', '.', 'from', ':', 'george', 'weissman', '12', '/', '10', '/', '99', '10', ':', '00', 'am', 'to', ':', 'daren', 'j', 'farmer', '/', 'hou', '/', 'ect', '@', 'ect', 'cc', ':', 'gary', 'bryan', '/', 'hou', '/', 'ect', '@', 'ect', ',', 'melissa', 'graves', '/', 'hou', '/', 'ect', '@', 'ect', 'subject', ':', 'vastar', 'resources', ',', 'inc', '.', 'darren', ',', 'the', 'attached', 'appears', 'to', 'be', 'a', 'nomination', 'from', 'vastar', 'resources', ',', 'inc', '.', 'for', 'the', 'high', 'island', 'larger', 'block', 'a', '-', '1', '#', '2', '(', 'previously', ',', 'erroneously', 'referred', 'to', 'as', 'the', '#', '1', 'well', ')', '.', 'vastar', 'now', 'expects', 'the', 'well', 'to', 'commence', 'production', 'sometime', 'tomorrow', '.', 'i', 'told', 'linda', 'harris', 'that', 'we', \"'\", 'd', 'get', 'her', 'a', 'telephone', 'number', 'in', 'gas', 'control', 'so', 'she', 'can', 'provide', 'notification', 'of', 'the', 'turn', '-', 'on', 'tomorrow', '.', 'linda', \"'\", 's', 'numbers', ',', 'for', 'the', 'record', ',', 'are', '281', '.', '584', '.', '3359', 'voice', 'and', '713', '.', '312', '.', '1689', 'fax', '.', 'would', 'you', 'please', 'see', 'that', 'someone', 'contacts', 'linda', 'and', 'advises', 'her', 'how', 'to', 'submit', 'future', 'nominations', 'via', 'e', '-', 'mail', ',', 'fax', 'or', 'voice', '?', 'thanks', '.', 'george', 'x', '3', '-', '6992', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'forwarded', 'by', 'george', 'weissman', '/', 'hou', '/', 'ect', 'on', '12', '/', '10', '/', '99', '09', ':', '44', 'am', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', \"''\", 'linda', 'harris', '``', 'on', '12', '/', '10', '/', '99', '09', ':', '38', ':', '43', 'am', 'to', ':', 'george', 'weissman', '/', 'hou', '/', 'ect', '@', 'ect', 'cc', ':', 'subject', ':', 'hi', 'a', '-', '1', '#', '2', 'effective', '12', '-', '11', '-', '99', '|', '-', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '|', '|', '|', '|', '|', '|', 'mscf', '/', 'd', '|', 'min', 'ftp', '|', 'time', '|', '|', '|', '|', '|', '|', '-', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '|', '|', '|', '|', '|', '|', '4', ',', '500', '|', '9', ',', '925', '|', '24', 'hours', '|', '|', '|', '|', '|', '|', '-', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '|', '|', '|', '|', '|', '|', '6', ',', '000', '|', '9', ',', '908', '|', '24', 'hours', '|', '|', '|', '|', '|', '|', '-', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '|', '|', '|', '|', '|', '|', '8', ',', '000', '|', '9', ',', '878', '|', '24', 'hours', '|', '|', '|', '|', '|', '|', '-', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '|', '|', '|', '|', '|', '|', '10', ',', '000', '|', '9', ',', '840', '|', '24', 'hours', '|', '|', '|', '|', '|', '|', '-', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '|', '|', '|', '|', '|', '|', '12', ',', '000', '|', '9', ',', '793', '|', '24', 'hours', '|', '|', '|', '|', '|', '|', '-', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '|', '|', '|', '|', '|', '|', '14', ',', '000', '|', '9', ',', '738', '|', '24', 'hours', '|', '|', '|', '|', '|', '|', '-', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '|', '|', '|', '|', '|', '|', '16', ',', '000', '|', '9', ',', '674', '|', '24', 'hours', '|', '|', '|', '|', '|', '|', '-', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '|', '|', '|', '|', '|', '|', '18', ',', '000', '|', '9', ',', '602', '|', '24', 'hours', '|', '|', '|', '|', '|', '|', '-', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '|', '|', '|', '|', '|', '|', '20', ',', '000', '|', '9', ',', '521', '|', '24', 'hours', '|', '|', '|', '|', '|', '|', '-', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '|', '|', '|', '|', '|', '|', '22', ',', '000', '|', '9', ',', '431', '|', '24', 'hours', '|', '|', '|', '|', '|', '|', '-', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '|', '|', '|', '|', '|', '|', '24', ',', '000', '|', '9', ',', '332', '|', '24', 'hours', '|', '|', '|', '|', '|', '|', '-', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '|', '|', '|', '|', '|', '|', '26', ',', '000', '|', '9', ',', '224', '|', '24', 'hours', '|', '|', '|', '|', '|', '|', '-', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '|', '|', '|', '|', '|', '|', '28', ',', '000', '|', '9', ',', '108', '|', '24', 'hours', '|', '|', '|', '|', '|', '|', '-', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '|', '|', '|', '|', '|', '|', '30', ',', '000', '|', '8', ',', '982', '|', '24', 'hours', '|', '|', '|', '|', '|', '|', '-', '-', '-', '-', '-', '-']\n"
     ]
    }
   ],
   "source": [
    "#Original amount of HAM tokens without any cleaning\n",
    "print(len(ExtractTokens(HAM)))\n",
    "print(ExtractTokens(HAM)[:1400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncleaned SPAM Tokens: 355375\n",
      "Uncleaned HAM Tokens: 830750\n"
     ]
    }
   ],
   "source": [
    "ORGSPAMTOKENS = ExtractTokens(SPAM)\n",
    "print(\"Uncleaned SPAM Tokens:\",len(ORGSPAMTOKENS)) \n",
    "ORGHAMTOKENS = ExtractTokens(HAM)\n",
    "print(\"Uncleaned HAM Tokens:\",len(ORGHAMTOKENS)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Subject',\n",
       " ':',\n",
       " 'dobmeos',\n",
       " 'with',\n",
       " 'hgh',\n",
       " 'my',\n",
       " 'energy',\n",
       " 'level',\n",
       " 'has',\n",
       " 'gone',\n",
       " 'up',\n",
       " '!',\n",
       " 'stukm',\n",
       " 'introducing',\n",
       " 'doctor',\n",
       " '-',\n",
       " 'formulated',\n",
       " 'hgh',\n",
       " 'human',\n",
       " 'growth',\n",
       " 'hormone',\n",
       " '-',\n",
       " 'also',\n",
       " 'called',\n",
       " 'hgh',\n",
       " 'is',\n",
       " 'referred',\n",
       " 'to',\n",
       " 'in',\n",
       " 'medical',\n",
       " 'science',\n",
       " 'as',\n",
       " 'the',\n",
       " 'master',\n",
       " 'hormone',\n",
       " '.',\n",
       " 'it',\n",
       " 'is',\n",
       " 'very',\n",
       " 'plentiful',\n",
       " 'when',\n",
       " 'we',\n",
       " 'are',\n",
       " 'young',\n",
       " ',',\n",
       " 'but',\n",
       " 'near',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " 'twenty',\n",
       " '-',\n",
       " 'one',\n",
       " 'our',\n",
       " 'bodies',\n",
       " 'begin',\n",
       " 'to',\n",
       " 'produce',\n",
       " 'less',\n",
       " 'of',\n",
       " 'it',\n",
       " '.',\n",
       " 'by',\n",
       " 'the',\n",
       " 'time',\n",
       " 'we',\n",
       " 'are',\n",
       " 'forty',\n",
       " 'nearly',\n",
       " 'everyone',\n",
       " 'is',\n",
       " 'deficient',\n",
       " 'in',\n",
       " 'hgh',\n",
       " ',',\n",
       " 'and',\n",
       " 'at',\n",
       " 'eighty',\n",
       " 'our',\n",
       " 'production',\n",
       " 'has',\n",
       " 'normally',\n",
       " 'diminished',\n",
       " 'at',\n",
       " 'least',\n",
       " '90',\n",
       " '-',\n",
       " '95',\n",
       " '%',\n",
       " '.',\n",
       " 'advantages',\n",
       " 'of',\n",
       " 'hgh',\n",
       " ':',\n",
       " '-',\n",
       " 'increased',\n",
       " 'muscle',\n",
       " 'strength',\n",
       " '-',\n",
       " 'loss']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ORGSPAMTOKENS[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Subject',\n",
       " ':',\n",
       " 'christmas',\n",
       " 'tree',\n",
       " 'farm',\n",
       " 'pictures',\n",
       " 'Subject',\n",
       " ':',\n",
       " 'vastar',\n",
       " 'resources',\n",
       " ',',\n",
       " 'inc',\n",
       " '.',\n",
       " 'gary',\n",
       " ',',\n",
       " 'production',\n",
       " 'from',\n",
       " 'the',\n",
       " 'high',\n",
       " 'island',\n",
       " 'larger',\n",
       " 'block',\n",
       " 'a',\n",
       " '-',\n",
       " '1',\n",
       " '#',\n",
       " '2',\n",
       " 'commenced',\n",
       " 'on',\n",
       " 'saturday',\n",
       " 'at',\n",
       " '2',\n",
       " ':',\n",
       " '00',\n",
       " 'p',\n",
       " '.',\n",
       " 'm',\n",
       " '.',\n",
       " 'at',\n",
       " 'about',\n",
       " '6',\n",
       " ',',\n",
       " '500',\n",
       " 'gross',\n",
       " '.',\n",
       " 'carlos',\n",
       " 'expects',\n",
       " 'between',\n",
       " '9',\n",
       " ',',\n",
       " '500',\n",
       " 'and',\n",
       " '10',\n",
       " ',',\n",
       " '000',\n",
       " 'gross',\n",
       " 'for',\n",
       " 'tomorrow',\n",
       " '.',\n",
       " 'vastar',\n",
       " 'owns',\n",
       " '68',\n",
       " '%',\n",
       " 'of',\n",
       " 'the',\n",
       " 'gross',\n",
       " 'production',\n",
       " '.',\n",
       " 'george',\n",
       " 'x',\n",
       " '3',\n",
       " '-',\n",
       " '6992',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " 'forwarded',\n",
       " 'by',\n",
       " 'george',\n",
       " 'weissman',\n",
       " '/']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ORGHAMTOKENS[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Subject', ':'), (':', 'dobmeos'), ('dobmeos', 'with'), ('with', 'hgh'), ('hgh', 'my'), ('my', 'energy'), ('energy', 'level'), ('level', 'has'), ('has', 'gone'), ('gone', 'up'), ('up', '!'), ('!', 'stukm'), ('stukm', 'introducing'), ('introducing', 'doctor'), ('doctor', '-'), ('-', 'formulated'), ('formulated', 'hgh'), ('hgh', 'human'), ('human', 'growth'), ('growth', 'hormone'), ('hormone', '-'), ('-', 'also'), ('also', 'called'), ('called', 'hgh'), ('hgh', 'is'), ('is', 'referred'), ('referred', 'to'), ('to', 'in'), ('in', 'medical'), ('medical', 'science'), ('science', 'as'), ('as', 'the'), ('the', 'master'), ('master', 'hormone'), ('hormone', '.'), ('.', 'it'), ('it', 'is'), ('is', 'very'), ('very', 'plentiful'), ('plentiful', 'when'), ('when', 'we'), ('we', 'are'), ('are', 'young'), ('young', ','), (',', 'but'), ('but', 'near'), ('near', 'the'), ('the', 'age'), ('age', 'of'), ('of', 'twenty'), ('twenty', '-'), ('-', 'one'), ('one', 'our'), ('our', 'bodies'), ('bodies', 'begin'), ('begin', 'to'), ('to', 'produce'), ('produce', 'less'), ('less', 'of'), ('of', 'it'), ('it', '.'), ('.', 'by'), ('by', 'the'), ('the', 'time'), ('time', 'we'), ('we', 'are'), ('are', 'forty'), ('forty', 'nearly'), ('nearly', 'everyone'), ('everyone', 'is'), ('is', 'deficient'), ('deficient', 'in'), ('in', 'hgh'), ('hgh', ','), (',', 'and'), ('and', 'at'), ('at', 'eighty'), ('eighty', 'our'), ('our', 'production'), ('production', 'has'), ('has', 'normally'), ('normally', 'diminished'), ('diminished', 'at'), ('at', 'least'), ('least', '90'), ('90', '-'), ('-', '95'), ('95', '%'), ('%', '.'), ('.', 'advantages'), ('advantages', 'of'), ('of', 'hgh'), ('hgh', ':'), (':', '-'), ('-', 'increased'), ('increased', 'muscle'), ('muscle', 'strength'), ('strength', '-'), ('-', 'loss'), ('loss', 'in'), ('in', 'body'), ('body', 'fat'), ('fat', '-'), ('-', 'increased'), ('increased', 'bone'), ('bone', 'density'), ('density', '-'), ('-', 'lower'), ('lower', 'blood'), ('blood', 'pressure'), ('pressure', '-'), ('-', 'quickens'), ('quickens', 'wound'), ('wound', 'healing'), ('healing', '-'), ('-', 'reduces'), ('reduces', 'cellulite'), ('cellulite', '-'), ('-', 'improved'), ('improved', 'vision'), ('vision', '-'), ('-', 'wrinkle'), ('wrinkle', 'disappearance'), ('disappearance', '-'), ('-', 'increased'), ('increased', 'skin'), ('skin', 'thickness'), ('thickness', 'texture'), ('texture', '-'), ('-', 'increased'), ('increased', 'energy'), ('energy', 'levels'), ('levels', '-'), ('-', 'improved'), ('improved', 'sleep'), ('sleep', 'and'), ('and', 'emotional'), ('emotional', 'stability'), ('stability', '-'), ('-', 'improved'), ('improved', 'memory'), ('memory', 'and'), ('and', 'mental'), ('mental', 'alertness'), ('alertness', '-'), ('-', 'increased'), ('increased', 'sexual'), ('sexual', 'potency'), ('potency', '-'), ('-', 'resistance')]\n"
     ]
    }
   ],
   "source": [
    "#Original SPAM Tokens - Bigrams#\n",
    "ORGSPAMTOKENSbigram = list(nltk.bigrams(ORGSPAMTOKENS))\n",
    "print(ORGSPAMTOKENSbigram[:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Subject', ':'), (':', 'christmas'), ('christmas', 'tree'), ('tree', 'farm'), ('farm', 'pictures'), ('pictures', 'Subject'), ('Subject', ':'), (':', 'vastar'), ('vastar', 'resources'), ('resources', ','), (',', 'inc'), ('inc', '.'), ('.', 'gary'), ('gary', ','), (',', 'production'), ('production', 'from'), ('from', 'the'), ('the', 'high'), ('high', 'island'), ('island', 'larger'), ('larger', 'block'), ('block', 'a'), ('a', '-'), ('-', '1'), ('1', '#'), ('#', '2'), ('2', 'commenced'), ('commenced', 'on'), ('on', 'saturday'), ('saturday', 'at'), ('at', '2'), ('2', ':'), (':', '00'), ('00', 'p'), ('p', '.'), ('.', 'm'), ('m', '.'), ('.', 'at'), ('at', 'about'), ('about', '6'), ('6', ','), (',', '500'), ('500', 'gross'), ('gross', '.'), ('.', 'carlos'), ('carlos', 'expects'), ('expects', 'between'), ('between', '9'), ('9', ','), (',', '500'), ('500', 'and'), ('and', '10'), ('10', ','), (',', '000'), ('000', 'gross'), ('gross', 'for'), ('for', 'tomorrow'), ('tomorrow', '.'), ('.', 'vastar'), ('vastar', 'owns'), ('owns', '68'), ('68', '%'), ('%', 'of'), ('of', 'the'), ('the', 'gross'), ('gross', 'production'), ('production', '.'), ('.', 'george'), ('george', 'x'), ('x', '3'), ('3', '-'), ('-', '6992'), ('6992', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', 'forwarded'), ('forwarded', 'by'), ('by', 'george'), ('george', 'weissman'), ('weissman', '/'), ('/', 'hou'), ('hou', '/'), ('/', 'ect'), ('ect', 'on'), ('on', '12'), ('12', '/'), ('/', '13'), ('13', '/'), ('/', '99'), ('99', '10'), ('10', ':'), (':', '16'), ('16', 'am'), ('am', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', '-'), ('-', 'daren'), ('daren', 'j'), ('j', 'farmer'), ('farmer', '12'), ('12', '/'), ('/', '10'), ('10', '/'), ('/', '99'), ('99', '10'), ('10', ':'), (':', '38')]\n"
     ]
    }
   ],
   "source": [
    "#Original HAM Tokens - Bigrams#\n",
    "ORGHAMTOKENSbigram = list(nltk.bigrams(ORGHAMTOKENS))\n",
    "print(ORGHAMTOKENSbigram[:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Subject', ':', 'dobmeos'), (':', 'dobmeos', 'with'), ('dobmeos', 'with', 'hgh'), ('with', 'hgh', 'my'), ('hgh', 'my', 'energy'), ('my', 'energy', 'level'), ('energy', 'level', 'has'), ('level', 'has', 'gone'), ('has', 'gone', 'up'), ('gone', 'up', '!'), ('up', '!', 'stukm'), ('!', 'stukm', 'introducing'), ('stukm', 'introducing', 'doctor'), ('introducing', 'doctor', '-'), ('doctor', '-', 'formulated'), ('-', 'formulated', 'hgh'), ('formulated', 'hgh', 'human'), ('hgh', 'human', 'growth'), ('human', 'growth', 'hormone'), ('growth', 'hormone', '-'), ('hormone', '-', 'also'), ('-', 'also', 'called'), ('also', 'called', 'hgh'), ('called', 'hgh', 'is'), ('hgh', 'is', 'referred'), ('is', 'referred', 'to'), ('referred', 'to', 'in'), ('to', 'in', 'medical'), ('in', 'medical', 'science'), ('medical', 'science', 'as'), ('science', 'as', 'the'), ('as', 'the', 'master'), ('the', 'master', 'hormone'), ('master', 'hormone', '.'), ('hormone', '.', 'it'), ('.', 'it', 'is'), ('it', 'is', 'very'), ('is', 'very', 'plentiful'), ('very', 'plentiful', 'when'), ('plentiful', 'when', 'we'), ('when', 'we', 'are'), ('we', 'are', 'young'), ('are', 'young', ','), ('young', ',', 'but'), (',', 'but', 'near'), ('but', 'near', 'the'), ('near', 'the', 'age'), ('the', 'age', 'of'), ('age', 'of', 'twenty'), ('of', 'twenty', '-'), ('twenty', '-', 'one'), ('-', 'one', 'our'), ('one', 'our', 'bodies'), ('our', 'bodies', 'begin'), ('bodies', 'begin', 'to'), ('begin', 'to', 'produce'), ('to', 'produce', 'less'), ('produce', 'less', 'of'), ('less', 'of', 'it'), ('of', 'it', '.'), ('it', '.', 'by'), ('.', 'by', 'the'), ('by', 'the', 'time'), ('the', 'time', 'we'), ('time', 'we', 'are'), ('we', 'are', 'forty'), ('are', 'forty', 'nearly'), ('forty', 'nearly', 'everyone'), ('nearly', 'everyone', 'is'), ('everyone', 'is', 'deficient'), ('is', 'deficient', 'in'), ('deficient', 'in', 'hgh'), ('in', 'hgh', ','), ('hgh', ',', 'and'), (',', 'and', 'at'), ('and', 'at', 'eighty'), ('at', 'eighty', 'our'), ('eighty', 'our', 'production'), ('our', 'production', 'has'), ('production', 'has', 'normally'), ('has', 'normally', 'diminished'), ('normally', 'diminished', 'at'), ('diminished', 'at', 'least'), ('at', 'least', '90'), ('least', '90', '-'), ('90', '-', '95'), ('-', '95', '%'), ('95', '%', '.'), ('%', '.', 'advantages'), ('.', 'advantages', 'of'), ('advantages', 'of', 'hgh'), ('of', 'hgh', ':'), ('hgh', ':', '-'), (':', '-', 'increased'), ('-', 'increased', 'muscle'), ('increased', 'muscle', 'strength'), ('muscle', 'strength', '-'), ('strength', '-', 'loss'), ('-', 'loss', 'in'), ('loss', 'in', 'body'), ('in', 'body', 'fat'), ('body', 'fat', '-'), ('fat', '-', 'increased'), ('-', 'increased', 'bone'), ('increased', 'bone', 'density'), ('bone', 'density', '-'), ('density', '-', 'lower'), ('-', 'lower', 'blood'), ('lower', 'blood', 'pressure'), ('blood', 'pressure', '-'), ('pressure', '-', 'quickens'), ('-', 'quickens', 'wound'), ('quickens', 'wound', 'healing'), ('wound', 'healing', '-'), ('healing', '-', 'reduces'), ('-', 'reduces', 'cellulite'), ('reduces', 'cellulite', '-'), ('cellulite', '-', 'improved'), ('-', 'improved', 'vision'), ('improved', 'vision', '-'), ('vision', '-', 'wrinkle'), ('-', 'wrinkle', 'disappearance'), ('wrinkle', 'disappearance', '-'), ('disappearance', '-', 'increased'), ('-', 'increased', 'skin'), ('increased', 'skin', 'thickness'), ('skin', 'thickness', 'texture'), ('thickness', 'texture', '-'), ('texture', '-', 'increased'), ('-', 'increased', 'energy'), ('increased', 'energy', 'levels'), ('energy', 'levels', '-'), ('levels', '-', 'improved'), ('-', 'improved', 'sleep'), ('improved', 'sleep', 'and'), ('sleep', 'and', 'emotional'), ('and', 'emotional', 'stability'), ('emotional', 'stability', '-'), ('stability', '-', 'improved'), ('-', 'improved', 'memory'), ('improved', 'memory', 'and'), ('memory', 'and', 'mental'), ('and', 'mental', 'alertness'), ('mental', 'alertness', '-'), ('alertness', '-', 'increased'), ('-', 'increased', 'sexual'), ('increased', 'sexual', 'potency'), ('sexual', 'potency', '-'), ('potency', '-', 'resistance'), ('-', 'resistance', 'to')]\n"
     ]
    }
   ],
   "source": [
    "#Original SPAM Tokens - Trigrams#\n",
    "ORGSPAMTOKENStrigram = list(nltk.trigrams(ORGSPAMTOKENS))\n",
    "print(ORGSPAMTOKENStrigram[:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Subject', ':', 'christmas'), (':', 'christmas', 'tree'), ('christmas', 'tree', 'farm'), ('tree', 'farm', 'pictures'), ('farm', 'pictures', 'Subject'), ('pictures', 'Subject', ':'), ('Subject', ':', 'vastar'), (':', 'vastar', 'resources'), ('vastar', 'resources', ','), ('resources', ',', 'inc'), (',', 'inc', '.'), ('inc', '.', 'gary'), ('.', 'gary', ','), ('gary', ',', 'production'), (',', 'production', 'from'), ('production', 'from', 'the'), ('from', 'the', 'high'), ('the', 'high', 'island'), ('high', 'island', 'larger'), ('island', 'larger', 'block'), ('larger', 'block', 'a'), ('block', 'a', '-'), ('a', '-', '1'), ('-', '1', '#'), ('1', '#', '2'), ('#', '2', 'commenced'), ('2', 'commenced', 'on'), ('commenced', 'on', 'saturday'), ('on', 'saturday', 'at'), ('saturday', 'at', '2'), ('at', '2', ':'), ('2', ':', '00'), (':', '00', 'p'), ('00', 'p', '.'), ('p', '.', 'm'), ('.', 'm', '.'), ('m', '.', 'at'), ('.', 'at', 'about'), ('at', 'about', '6'), ('about', '6', ','), ('6', ',', '500'), (',', '500', 'gross'), ('500', 'gross', '.'), ('gross', '.', 'carlos'), ('.', 'carlos', 'expects'), ('carlos', 'expects', 'between'), ('expects', 'between', '9'), ('between', '9', ','), ('9', ',', '500'), (',', '500', 'and'), ('500', 'and', '10'), ('and', '10', ','), ('10', ',', '000'), (',', '000', 'gross'), ('000', 'gross', 'for'), ('gross', 'for', 'tomorrow'), ('for', 'tomorrow', '.'), ('tomorrow', '.', 'vastar'), ('.', 'vastar', 'owns'), ('vastar', 'owns', '68'), ('owns', '68', '%'), ('68', '%', 'of'), ('%', 'of', 'the'), ('of', 'the', 'gross'), ('the', 'gross', 'production'), ('gross', 'production', '.'), ('production', '.', 'george'), ('.', 'george', 'x'), ('george', 'x', '3'), ('x', '3', '-'), ('3', '-', '6992'), ('-', '6992', '-'), ('6992', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', 'forwarded'), ('-', 'forwarded', 'by'), ('forwarded', 'by', 'george'), ('by', 'george', 'weissman'), ('george', 'weissman', '/'), ('weissman', '/', 'hou'), ('/', 'hou', '/'), ('hou', '/', 'ect'), ('/', 'ect', 'on'), ('ect', 'on', '12'), ('on', '12', '/'), ('12', '/', '13'), ('/', '13', '/'), ('13', '/', '99'), ('/', '99', '10'), ('99', '10', ':'), ('10', ':', '16'), (':', '16', 'am'), ('16', 'am', '-'), ('am', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', '-'), ('-', '-', 'daren'), ('-', 'daren', 'j'), ('daren', 'j', 'farmer'), ('j', 'farmer', '12'), ('farmer', '12', '/'), ('12', '/', '10'), ('/', '10', '/'), ('10', '/', '99'), ('/', '99', '10'), ('99', '10', ':'), ('10', ':', '38'), (':', '38', 'am')]\n"
     ]
    }
   ],
   "source": [
    "#Original HAM Tokens - Trigrams#\n",
    "ORGHAMTOKENStrigram = list(nltk.trigrams(ORGHAMTOKENS))\n",
    "print(ORGHAMTOKENStrigram[:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to remove punctuation and non-alphabetic characters#\n",
    "def remove_punc_filter(word):\n",
    "\n",
    "    mPattern = re.compile('^[^a-z]+$')\n",
    "    if (mPattern.match(word)):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['subject',\n",
       " 'dobmeos',\n",
       " 'with',\n",
       " 'hgh',\n",
       " 'my',\n",
       " 'energy',\n",
       " 'level',\n",
       " 'has',\n",
       " 'gone',\n",
       " 'up',\n",
       " 'stukm',\n",
       " 'introducing',\n",
       " 'doctor',\n",
       " 'formulated',\n",
       " 'hgh',\n",
       " 'human',\n",
       " 'growth',\n",
       " 'hormone',\n",
       " 'also',\n",
       " 'called',\n",
       " 'hgh',\n",
       " 'is',\n",
       " 'referred',\n",
       " 'to',\n",
       " 'in',\n",
       " 'medical',\n",
       " 'science',\n",
       " 'as',\n",
       " 'the',\n",
       " 'master',\n",
       " 'hormone',\n",
       " 'it',\n",
       " 'is',\n",
       " 'very',\n",
       " 'plentiful',\n",
       " 'when',\n",
       " 'we',\n",
       " 'are',\n",
       " 'young',\n",
       " 'but',\n",
       " 'near',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " 'twenty',\n",
       " 'one',\n",
       " 'our',\n",
       " 'bodies',\n",
       " 'begin',\n",
       " 'to']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use created function and apply to Original SPAM Tokens. Also convert to all lowercase#\n",
    "ORGSPAMTOKENSpunc = [w for w in ORGSPAMTOKENS if not remove_punc_filter(w)]\n",
    "ORGSPAMTOKENSpunc = [w.lower() for w in ORGSPAMTOKENSpunc]\n",
    "ORGSPAMTOKENSpunc[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['subject',\n",
       " 'christmas',\n",
       " 'tree',\n",
       " 'farm',\n",
       " 'pictures',\n",
       " 'subject',\n",
       " 'vastar',\n",
       " 'resources',\n",
       " 'inc',\n",
       " 'gary',\n",
       " 'production',\n",
       " 'from',\n",
       " 'the',\n",
       " 'high',\n",
       " 'island',\n",
       " 'larger',\n",
       " 'block',\n",
       " 'a',\n",
       " 'commenced',\n",
       " 'on',\n",
       " 'saturday',\n",
       " 'at',\n",
       " 'p',\n",
       " 'm',\n",
       " 'at',\n",
       " 'about',\n",
       " 'gross',\n",
       " 'carlos',\n",
       " 'expects',\n",
       " 'between',\n",
       " 'and',\n",
       " 'gross',\n",
       " 'for',\n",
       " 'tomorrow',\n",
       " 'vastar',\n",
       " 'owns',\n",
       " 'of',\n",
       " 'the',\n",
       " 'gross',\n",
       " 'production',\n",
       " 'george',\n",
       " 'x',\n",
       " 'forwarded',\n",
       " 'by',\n",
       " 'george',\n",
       " 'weissman',\n",
       " 'hou',\n",
       " 'ect',\n",
       " 'on',\n",
       " 'am']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use created function and apply to Original HAM Tokens. Also convert to all lowercase#\n",
    "ORGHAMTOKENSpunc = [w for w in ORGHAMTOKENS if not remove_punc_filter(w)]\n",
    "ORGHAMTOKENSpunc = [w.lower() for w in ORGHAMTOKENSpunc]\n",
    "ORGHAMTOKENSpunc[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORGSPAMTOKENSpuncFD = FreqDist(ORGSPAMTOKENSpunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 7297)\n",
      "('to', 5173)\n",
      "('and', 4917)\n",
      "('of', 4511)\n",
      "('a', 3794)\n",
      "('in', 3147)\n",
      "('you', 2797)\n",
      "('for', 2526)\n",
      "('this', 2293)\n",
      "('is', 2264)\n",
      "('your', 1952)\n",
      "('subject', 1658)\n",
      "('with', 1471)\n",
      "('that', 1350)\n",
      "('s', 1324)\n",
      "('be', 1310)\n",
      "('or', 1303)\n",
      "('on', 1268)\n",
      "('as', 1201)\n",
      "('are', 1168)\n",
      "('i', 1150)\n",
      "('we', 1139)\n",
      "('it', 1089)\n",
      "('not', 1078)\n",
      "('our', 1037)\n",
      "('com', 993)\n",
      "('http', 983)\n",
      "('from', 982)\n",
      "('have', 888)\n",
      "('all', 885)\n",
      "('no', 757)\n",
      "('at', 744)\n",
      "('company', 731)\n",
      "('will', 696)\n",
      "('by', 671)\n",
      "('can', 650)\n",
      "('e', 638)\n",
      "('an', 608)\n",
      "('more', 603)\n",
      "('here', 588)\n",
      "('www', 587)\n",
      "('any', 555)\n",
      "('if', 549)\n",
      "('information', 520)\n",
      "('font', 515)\n",
      "('me', 513)\n",
      "('t', 509)\n",
      "('only', 508)\n",
      "('td', 504)\n",
      "('has', 493)\n"
     ]
    }
   ],
   "source": [
    "#Return the 50 most frequent with Stopwords included#\n",
    "ORGSPAMTOKENStop50 = ORGSPAMTOKENSpuncFD.most_common(50)\n",
    "#Print out Top 50#\n",
    "for records in ORGSPAMTOKENStop50:\n",
    "  print(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORGHAMTOKENSpuncFD = FreqDist(ORGHAMTOKENSpunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 18359)\n",
      "('to', 15172)\n",
      "('ect', 13897)\n",
      "('for', 7982)\n",
      "('and', 7912)\n",
      "('hou', 7281)\n",
      "('enron', 6555)\n",
      "('subject', 6403)\n",
      "('on', 6049)\n",
      "('a', 6026)\n",
      "('of', 5677)\n",
      "('you', 5365)\n",
      "('i', 5241)\n",
      "('is', 4906)\n",
      "('this', 4878)\n",
      "('in', 4570)\n",
      "('be', 3757)\n",
      "('will', 3441)\n",
      "('that', 3419)\n",
      "('from', 3210)\n",
      "('have', 3209)\n",
      "('we', 3202)\n",
      "('at', 2991)\n",
      "('gas', 2861)\n",
      "('deal', 2789)\n",
      "('com', 2717)\n",
      "('please', 2715)\n",
      "('if', 2586)\n",
      "('with', 2516)\n",
      "('meter', 2459)\n",
      "('am', 2405)\n",
      "('cc', 2359)\n",
      "('by', 2329)\n",
      "('pm', 2325)\n",
      "('hpl', 2318)\n",
      "('it', 2246)\n",
      "('are', 2220)\n",
      "('s', 2111)\n",
      "('your', 2090)\n",
      "('me', 2059)\n",
      "('not', 1996)\n",
      "('as', 1956)\n",
      "('daren', 1901)\n",
      "('thanks', 1813)\n",
      "('or', 1777)\n",
      "('re', 1753)\n",
      "('corp', 1710)\n",
      "('d', 1658)\n",
      "('can', 1493)\n",
      "('any', 1445)\n"
     ]
    }
   ],
   "source": [
    "#Return the 50 most frequent with Stopwords included#\n",
    "ORGHAMTOKENStop50 = ORGHAMTOKENSpuncFD.most_common(50)\n",
    "#Print out Top 50#\n",
    "for records in ORGHAMTOKENStop50:\n",
    "  print(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nostopORGSPAMTOKENS = [w for w in ORGSPAMTOKENSpunc if not w in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179807"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nostopORGSPAMTOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nostopORGSPAMTOKENSFD = FreqDist(nostopORGSPAMTOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('subject', 1658)\n",
      "('com', 993)\n",
      "('http', 983)\n",
      "('company', 731)\n",
      "('e', 638)\n",
      "('www', 587)\n",
      "('information', 520)\n",
      "('font', 515)\n",
      "('td', 504)\n",
      "('get', 486)\n",
      "('please', 485)\n",
      "('statements', 476)\n",
      "('email', 475)\n",
      "('us', 471)\n",
      "('price', 471)\n",
      "('new', 434)\n",
      "('may', 423)\n",
      "('nbsp', 418)\n",
      "('one', 392)\n",
      "('p', 391)\n",
      "('height', 362)\n",
      "('time', 361)\n",
      "('free', 314)\n",
      "('within', 313)\n",
      "('pills', 311)\n",
      "('r', 306)\n",
      "('size', 306)\n",
      "('width', 306)\n",
      "('stock', 299)\n",
      "('b', 298)\n",
      "('message', 297)\n",
      "('money', 295)\n",
      "('investment', 290)\n",
      "('c', 283)\n",
      "('u', 283)\n",
      "('report', 282)\n",
      "('inc', 268)\n",
      "('securities', 263)\n",
      "('business', 258)\n",
      "('online', 257)\n",
      "('click', 256)\n",
      "('looking', 254)\n",
      "('best', 254)\n",
      "('mail', 245)\n",
      "('contact', 243)\n",
      "('like', 243)\n",
      "('computron', 242)\n",
      "('prices', 239)\n",
      "('align', 233)\n",
      "('future', 232)\n"
     ]
    }
   ],
   "source": [
    "#Return the 50 most frequent with Stopwords removed#\n",
    "nostopORGSPAMTOKENStop50 = nostopORGSPAMTOKENSFD.most_common(50)\n",
    "#Print out Top 50#\n",
    "for records in nostopORGSPAMTOKENStop50:\n",
    "  print(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nostopORGHAMTOKENS = [w for w in ORGHAMTOKENSpunc if not w in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "323901"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nostopORGHAMTOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nostopORGHAMTOKENSFD = FreqDist(nostopORGHAMTOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ect', 13897)\n",
      "('hou', 7281)\n",
      "('enron', 6555)\n",
      "('subject', 6403)\n",
      "('gas', 2861)\n",
      "('deal', 2789)\n",
      "('com', 2717)\n",
      "('please', 2715)\n",
      "('meter', 2459)\n",
      "('cc', 2359)\n",
      "('pm', 2325)\n",
      "('hpl', 2318)\n",
      "('daren', 1901)\n",
      "('thanks', 1813)\n",
      "('corp', 1710)\n",
      "('know', 1438)\n",
      "('mmbtu', 1349)\n",
      "('e', 1338)\n",
      "('j', 1300)\n",
      "('forwarded', 1296)\n",
      "('need', 1257)\n",
      "('farmer', 1137)\n",
      "('let', 1086)\n",
      "('attached', 1083)\n",
      "('xls', 1020)\n",
      "('see', 1018)\n",
      "('new', 1003)\n",
      "('may', 960)\n",
      "('l', 923)\n",
      "('contract', 883)\n",
      "('volume', 877)\n",
      "('would', 875)\n",
      "('robert', 875)\n",
      "('day', 874)\n",
      "('sitara', 861)\n",
      "('nom', 831)\n",
      "('texas', 796)\n",
      "('get', 790)\n",
      "('volumes', 790)\n",
      "('month', 780)\n",
      "('questions', 760)\n",
      "('pec', 752)\n",
      "('deals', 745)\n",
      "('price', 735)\n",
      "('ena', 732)\n",
      "('bob', 706)\n",
      "('th', 706)\n",
      "('flow', 673)\n",
      "('message', 669)\n",
      "('energy', 664)\n"
     ]
    }
   ],
   "source": [
    "#Return the 50 most frequent with Stopwords removed#\n",
    "nostopORGHAMTOKENStop50 = nostopORGHAMTOKENSFD.most_common(50)\n",
    "#Print out Top 50#\n",
    "for records in nostopORGHAMTOKENStop50:\n",
    "  print(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Words in SPAM and HAM: 100\n",
      "Common Words in SPAM and HAM: 73\n",
      "Common Words in SPAM and HAM: 27\n"
     ]
    }
   ],
   "source": [
    "#Look for commonality between SPAM and HAM#\n",
    "SPAMmccount = nostopORGSPAMTOKENSFD.most_common(100)\n",
    "SPAMmc = set([entry[0] for entry in SPAMmccount])\n",
    "HAMmccount = nostopORGHAMTOKENSFD.most_common(100)\n",
    "HAMmc = set([entry[0] for entry in HAMmccount])\n",
    "\n",
    "SmH = SPAMmc - HAMmc\n",
    "HmS = HAMmc - SPAMmc\n",
    "Common = HAMmc.intersection(SPAMmc)\n",
    "\n",
    "print(\"Common Words in SPAM and HAM: {}\".format(len(SPAMmc)))\n",
    "print(\"Common Words in SPAM and HAM: {}\".format(len(HmS)))\n",
    "print(\"Common Words in SPAM and HAM: {}\".format(len(Common)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'also',\n",
       " 'com',\n",
       " 'company',\n",
       " 'could',\n",
       " 'e',\n",
       " 'gas',\n",
       " 'get',\n",
       " 'information',\n",
       " 'l',\n",
       " 'like',\n",
       " 'mail',\n",
       " 'may',\n",
       " 'message',\n",
       " 'need',\n",
       " 'net',\n",
       " 'new',\n",
       " 'one',\n",
       " 'p',\n",
       " 'please',\n",
       " 'price',\n",
       " 'see',\n",
       " 'subject',\n",
       " 'th',\n",
       " 'time',\n",
       " 'us',\n",
       " 'would',\n",
       " 'x'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine the tokens and identify those that should be added to the stopwords list#\n",
    "SPAMstopwords = [\"also\", \"com\", \"company\", \"could\", \"e\", \"gas\", \"get\",\"information\", \"l\",\"like\",\"mail\",\"may\",\"message\",\"need\",\n",
    "                        \"net\",\"new\",\"one\",\"p\",\"please\",\"price\",\"see\",\"subject\",\"th\",\"time\",\"us\",\"would\",\"x\"]\n",
    "SPAMtotalstop = stopwords + SPAMstopwords\n",
    "len(SPAMtotalstop)\n",
    "nostopSPAMTOKENS = [w for w in ORGSPAMTOKENSpunc if not w in SPAMtotalstop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine the tokens and identify those that should be added to the stopwords list#\n",
    "HAMstopwords = [\"also\", \"com\", \"company\", \"could\", \"e\", \"gas\", \"get\",\"information\", \"l\",\"like\",\"mail\",\"may\",\"message\",\"need\",\n",
    "                        \"net\",\"new\",\"one\",\"p\",\"please\",\"price\",\"see\",\"subject\",\"th\",\"time\",\"us\",\"would\",\"x\"]\n",
    "HAMtotalstop = stopwords + HAMstopwords\n",
    "len(HAMtotalstop)\n",
    "nostopHAMTOKENS = [w for w in ORGHAMTOKENSpunc if not w in HAMtotalstop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "LengthnostopSPAMTOKENS = len(nostopSPAMTOKENS)\n",
    "LengthnostopHAMTOKENS = len(nostopHAMTOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168557"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LengthnostopSPAMTOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292733"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LengthnostopHAMTOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "nostopSPAMTOKENSFD = FreqDist(nostopSPAMTOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('http', 983)\n",
      "('www', 587)\n",
      "('font', 515)\n",
      "('td', 504)\n",
      "('statements', 476)\n",
      "('email', 475)\n",
      "('nbsp', 418)\n",
      "('height', 362)\n",
      "('free', 314)\n",
      "('within', 313)\n",
      "('pills', 311)\n",
      "('r', 306)\n",
      "('size', 306)\n",
      "('width', 306)\n",
      "('stock', 299)\n",
      "('b', 298)\n",
      "('money', 295)\n",
      "('investment', 290)\n",
      "('c', 283)\n",
      "('u', 283)\n",
      "('report', 282)\n",
      "('inc', 268)\n",
      "('securities', 263)\n",
      "('business', 258)\n",
      "('online', 257)\n",
      "('click', 256)\n",
      "('looking', 254)\n",
      "('best', 254)\n",
      "('contact', 243)\n",
      "('computron', 242)\n",
      "('prices', 239)\n",
      "('align', 233)\n",
      "('future', 232)\n",
      "('tr', 228)\n",
      "('news', 224)\n",
      "('products', 222)\n",
      "('v', 222)\n",
      "('save', 215)\n",
      "('go', 213)\n",
      "('want', 213)\n",
      "('forward', 212)\n",
      "('n', 212)\n",
      "('many', 211)\n",
      "('use', 211)\n",
      "('windows', 207)\n",
      "('microsoft', 207)\n",
      "('today', 204)\n",
      "('color', 203)\n",
      "('million', 201)\n",
      "('international', 200)\n"
     ]
    }
   ],
   "source": [
    "#Return the 50 most frequent with Stopwords removed#\n",
    "nostopSPAMTOKENStop50 = nostopSPAMTOKENSFD.most_common(50)\n",
    "#Print out Top 50#\n",
    "for records in nostopSPAMTOKENStop50:\n",
    "  print(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "nostopHAMTOKENSFD = FreqDist(nostopHAMTOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ect', 13897)\n",
      "('hou', 7281)\n",
      "('enron', 6555)\n",
      "('deal', 2789)\n",
      "('meter', 2459)\n",
      "('cc', 2359)\n",
      "('pm', 2325)\n",
      "('hpl', 2318)\n",
      "('daren', 1901)\n",
      "('thanks', 1813)\n",
      "('corp', 1710)\n",
      "('know', 1438)\n",
      "('mmbtu', 1349)\n",
      "('j', 1300)\n",
      "('forwarded', 1296)\n",
      "('farmer', 1137)\n",
      "('let', 1086)\n",
      "('attached', 1083)\n",
      "('xls', 1020)\n",
      "('contract', 883)\n",
      "('volume', 877)\n",
      "('robert', 875)\n",
      "('day', 874)\n",
      "('sitara', 861)\n",
      "('nom', 831)\n",
      "('texas', 796)\n",
      "('volumes', 790)\n",
      "('month', 780)\n",
      "('questions', 760)\n",
      "('pec', 752)\n",
      "('deals', 745)\n",
      "('ena', 732)\n",
      "('bob', 706)\n",
      "('flow', 673)\n",
      "('energy', 664)\n",
      "('file', 652)\n",
      "('change', 647)\n",
      "('production', 639)\n",
      "('sent', 637)\n",
      "('call', 619)\n",
      "('following', 611)\n",
      "('nomination', 584)\n",
      "('gary', 561)\n",
      "('ticket', 560)\n",
      "('daily', 555)\n",
      "('mary', 550)\n",
      "('march', 514)\n",
      "('april', 512)\n",
      "('july', 507)\n",
      "('original', 506)\n"
     ]
    }
   ],
   "source": [
    "#Return the 50 most frequent with Stopwords removed#\n",
    "nostopHAMTOKENStop50 = nostopHAMTOKENSFD.most_common(50)\n",
    "#Print out Top 50#\n",
    "for records in nostopHAMTOKENStop50:\n",
    "  print(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words:  http \t\tCount:  983 \t\tPercent of SPAM Tokens:  0.0058318550994619035\n",
      "Words:  www \t\tCount:  587 \t\tPercent of SPAM Tokens:  0.0034825014683460193\n",
      "Words:  font \t\tCount:  515 \t\tPercent of SPAM Tokens:  0.003055346262688586\n",
      "Words:  td \t\tCount:  504 \t\tPercent of SPAM Tokens:  0.002990086439602034\n",
      "Words:  statements \t\tCount:  476 \t\tPercent of SPAM Tokens:  0.00282397052629081\n",
      "Words:  email \t\tCount:  475 \t\tPercent of SPAM Tokens:  0.002818037815101123\n",
      "Words:  nbsp \t\tCount:  418 \t\tPercent of SPAM Tokens:  0.0024798732772889883\n",
      "Words:  height \t\tCount:  362 \t\tPercent of SPAM Tokens:  0.00214764145066654\n",
      "Words:  free \t\tCount:  314 \t\tPercent of SPAM Tokens:  0.0018628713135615846\n",
      "Words:  within \t\tCount:  313 \t\tPercent of SPAM Tokens:  0.0018569386023718978\n",
      "Words:  pills \t\tCount:  311 \t\tPercent of SPAM Tokens:  0.0018450731799925248\n",
      "Words:  r \t\tCount:  306 \t\tPercent of SPAM Tokens:  0.0018154096240440918\n",
      "Words:  size \t\tCount:  306 \t\tPercent of SPAM Tokens:  0.0018154096240440918\n",
      "Words:  width \t\tCount:  306 \t\tPercent of SPAM Tokens:  0.0018154096240440918\n",
      "Words:  stock \t\tCount:  299 \t\tPercent of SPAM Tokens:  0.0017738806457162858\n",
      "Words:  b \t\tCount:  298 \t\tPercent of SPAM Tokens:  0.0017679479345265993\n",
      "Words:  money \t\tCount:  295 \t\tPercent of SPAM Tokens:  0.0017501498009575395\n",
      "Words:  investment \t\tCount:  290 \t\tPercent of SPAM Tokens:  0.0017204862450091068\n",
      "Words:  c \t\tCount:  283 \t\tPercent of SPAM Tokens:  0.0016789572666813008\n",
      "Words:  u \t\tCount:  283 \t\tPercent of SPAM Tokens:  0.0016789572666813008\n",
      "Words:  report \t\tCount:  282 \t\tPercent of SPAM Tokens:  0.001673024555491614\n",
      "Words:  inc \t\tCount:  268 \t\tPercent of SPAM Tokens:  0.001589966598836002\n",
      "Words:  securities \t\tCount:  263 \t\tPercent of SPAM Tokens:  0.0015603030428875692\n",
      "Words:  business \t\tCount:  258 \t\tPercent of SPAM Tokens:  0.0015306394869391362\n",
      "Words:  online \t\tCount:  257 \t\tPercent of SPAM Tokens:  0.0015247067757494497\n",
      "Words:  click \t\tCount:  256 \t\tPercent of SPAM Tokens:  0.0015187740645597632\n",
      "Words:  looking \t\tCount:  254 \t\tPercent of SPAM Tokens:  0.00150690864218039\n",
      "Words:  best \t\tCount:  254 \t\tPercent of SPAM Tokens:  0.00150690864218039\n",
      "Words:  contact \t\tCount:  243 \t\tPercent of SPAM Tokens:  0.0014416488190938377\n",
      "Words:  computron \t\tCount:  242 \t\tPercent of SPAM Tokens:  0.0014357161079041512\n",
      "Words:  prices \t\tCount:  239 \t\tPercent of SPAM Tokens:  0.0014179179743350914\n",
      "Words:  align \t\tCount:  233 \t\tPercent of SPAM Tokens:  0.001382321707196972\n",
      "Words:  future \t\tCount:  232 \t\tPercent of SPAM Tokens:  0.0013763889960072854\n",
      "Words:  tr \t\tCount:  228 \t\tPercent of SPAM Tokens:  0.0013526581512485392\n",
      "Words:  news \t\tCount:  224 \t\tPercent of SPAM Tokens:  0.0013289273064897927\n",
      "Words:  products \t\tCount:  222 \t\tPercent of SPAM Tokens:  0.0013170618841104197\n",
      "Words:  v \t\tCount:  222 \t\tPercent of SPAM Tokens:  0.0013170618841104197\n",
      "Words:  save \t\tCount:  215 \t\tPercent of SPAM Tokens:  0.0012755329057826136\n",
      "Words:  go \t\tCount:  213 \t\tPercent of SPAM Tokens:  0.0012636674834032404\n",
      "Words:  want \t\tCount:  213 \t\tPercent of SPAM Tokens:  0.0012636674834032404\n",
      "Words:  forward \t\tCount:  212 \t\tPercent of SPAM Tokens:  0.0012577347722135539\n",
      "Words:  n \t\tCount:  212 \t\tPercent of SPAM Tokens:  0.0012577347722135539\n",
      "Words:  many \t\tCount:  211 \t\tPercent of SPAM Tokens:  0.0012518020610238674\n",
      "Words:  use \t\tCount:  211 \t\tPercent of SPAM Tokens:  0.0012518020610238674\n",
      "Words:  windows \t\tCount:  207 \t\tPercent of SPAM Tokens:  0.001228071216265121\n",
      "Words:  microsoft \t\tCount:  207 \t\tPercent of SPAM Tokens:  0.001228071216265121\n",
      "Words:  today \t\tCount:  204 \t\tPercent of SPAM Tokens:  0.0012102730826960614\n",
      "Words:  color \t\tCount:  203 \t\tPercent of SPAM Tokens:  0.0012043403715063746\n",
      "Words:  million \t\tCount:  201 \t\tPercent of SPAM Tokens:  0.0011924749491270016\n",
      "Words:  international \t\tCount:  200 \t\tPercent of SPAM Tokens:  0.0011865422379373149\n"
     ]
    }
   ],
   "source": [
    "for records in nostopSPAMTOKENStop50:\n",
    "    #print(records[0], records[1], records[1]/LengthnostopORGSPAMTOKENS)\n",
    "    print(\"Words: \", (records[0]),\"\\t\\tCount: \", records[1], \"\\t\\tPercent of SPAM Tokens: \", records[1]/LengthnostopSPAMTOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words:  ect \t\tCount:  13897 \t\tPercent of HAM Tokens:  0.04747329477715188\n",
      "Words:  hou \t\tCount:  7281 \t\tPercent of HAM Tokens:  0.024872494730693157\n",
      "Words:  enron \t\tCount:  6555 \t\tPercent of HAM Tokens:  0.022392419030310896\n",
      "Words:  deal \t\tCount:  2789 \t\tPercent of HAM Tokens:  0.009527453344856917\n",
      "Words:  meter \t\tCount:  2459 \t\tPercent of HAM Tokens:  0.008400146208319527\n",
      "Words:  cc \t\tCount:  2359 \t\tPercent of HAM Tokens:  0.008058537985126377\n",
      "Words:  pm \t\tCount:  2325 \t\tPercent of HAM Tokens:  0.007942391189240708\n",
      "Words:  hpl \t\tCount:  2318 \t\tPercent of HAM Tokens:  0.007918478613617186\n",
      "Words:  daren \t\tCount:  1901 \t\tPercent of HAM Tokens:  0.006493972322901757\n",
      "Words:  thanks \t\tCount:  1813 \t\tPercent of HAM Tokens:  0.006193357086491786\n",
      "Words:  corp \t\tCount:  1710 \t\tPercent of HAM Tokens:  0.005841500616602843\n",
      "Words:  know \t\tCount:  1438 \t\tPercent of HAM Tokens:  0.004912326249517479\n",
      "Words:  mmbtu \t\tCount:  1349 \t\tPercent of HAM Tokens:  0.004608294930875576\n",
      "Words:  j \t\tCount:  1300 \t\tPercent of HAM Tokens:  0.004440906901510933\n",
      "Words:  forwarded \t\tCount:  1296 \t\tPercent of HAM Tokens:  0.004427242572583207\n",
      "Words:  farmer \t\tCount:  1137 \t\tPercent of HAM Tokens:  0.003884085497706101\n",
      "Words:  let \t\tCount:  1086 \t\tPercent of HAM Tokens:  0.003709865303877595\n",
      "Words:  attached \t\tCount:  1083 \t\tPercent of HAM Tokens:  0.0036996170571818006\n",
      "Words:  xls \t\tCount:  1020 \t\tPercent of HAM Tokens:  0.003484403876570117\n",
      "Words:  contract \t\tCount:  883 \t\tPercent of HAM Tokens:  0.003016400610795503\n",
      "Words:  volume \t\tCount:  877 \t\tPercent of HAM Tokens:  0.002995904117403914\n",
      "Words:  robert \t\tCount:  875 \t\tPercent of HAM Tokens:  0.0029890719529400514\n",
      "Words:  day \t\tCount:  874 \t\tPercent of HAM Tokens:  0.0029856558707081196\n",
      "Words:  sitara \t\tCount:  861 \t\tPercent of HAM Tokens:  0.0029412468016930104\n",
      "Words:  nom \t\tCount:  831 \t\tPercent of HAM Tokens:  0.0028387643347350658\n",
      "Words:  texas \t\tCount:  796 \t\tPercent of HAM Tokens:  0.0027192014566174635\n",
      "Words:  volumes \t\tCount:  790 \t\tPercent of HAM Tokens:  0.0026987049632258746\n",
      "Words:  month \t\tCount:  780 \t\tPercent of HAM Tokens:  0.00266454414090656\n",
      "Words:  questions \t\tCount:  760 \t\tPercent of HAM Tokens:  0.00259622249626793\n",
      "Words:  pec \t\tCount:  752 \t\tPercent of HAM Tokens:  0.0025688938384124784\n",
      "Words:  deals \t\tCount:  745 \t\tPercent of HAM Tokens:  0.0025449812627889577\n",
      "Words:  ena \t\tCount:  732 \t\tPercent of HAM Tokens:  0.0025005721937738485\n",
      "Words:  bob \t\tCount:  706 \t\tPercent of HAM Tokens:  0.0024117540557436297\n",
      "Words:  flow \t\tCount:  673 \t\tPercent of HAM Tokens:  0.002299023342089891\n",
      "Words:  energy \t\tCount:  664 \t\tPercent of HAM Tokens:  0.0022682786020025076\n",
      "Words:  file \t\tCount:  652 \t\tPercent of HAM Tokens:  0.0022272856152193298\n",
      "Words:  change \t\tCount:  647 \t\tPercent of HAM Tokens:  0.002210205204059672\n",
      "Words:  production \t\tCount:  639 \t\tPercent of HAM Tokens:  0.00218287654620422\n",
      "Words:  sent \t\tCount:  637 \t\tPercent of HAM Tokens:  0.0021760443817403574\n",
      "Words:  call \t\tCount:  619 \t\tPercent of HAM Tokens:  0.0021145549015655907\n",
      "Words:  following \t\tCount:  611 \t\tPercent of HAM Tokens:  0.0020872262437101386\n",
      "Words:  nomination \t\tCount:  584 \t\tPercent of HAM Tokens:  0.0019949920234479885\n",
      "Words:  gary \t\tCount:  561 \t\tPercent of HAM Tokens:  0.0019164221321135643\n",
      "Words:  ticket \t\tCount:  560 \t\tPercent of HAM Tokens:  0.0019130060498816328\n",
      "Words:  daily \t\tCount:  555 \t\tPercent of HAM Tokens:  0.0018959256387219754\n",
      "Words:  mary \t\tCount:  550 \t\tPercent of HAM Tokens:  0.0018788452275623178\n",
      "Words:  march \t\tCount:  514 \t\tPercent of HAM Tokens:  0.0017558662672127843\n",
      "Words:  april \t\tCount:  512 \t\tPercent of HAM Tokens:  0.0017490341027489214\n",
      "Words:  july \t\tCount:  507 \t\tPercent of HAM Tokens:  0.001731953691589264\n",
      "Words:  original \t\tCount:  506 \t\tPercent of HAM Tokens:  0.0017285376093573324\n"
     ]
    }
   ],
   "source": [
    "for records in nostopHAMTOKENStop50:\n",
    "    #print(records[0], records[1], records[1]/LengthnostopORGHAMTOKENS)\n",
    "    print(\"Words: \", records[0],\"\\t\\tCount: \", records[1], \"\\t\\tPercent of HAM Tokens: \", records[1]/LengthnostopHAMTOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Bigram Association Measure#\n",
    "Bigrammeasures = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words:  ('http', 'www') \t\tScore:  0.002426478876581809\n",
      "Words:  ('nbsp', 'nbsp') \t\tScore:  0.001756082512147226\n",
      "Words:  ('href', 'http') \t\tScore:  0.0010382244581951506\n",
      "Words:  ('looking', 'statements') \t\tScore:  0.0010204263246260908\n",
      "Words:  ('pills', 'pills') \t\tScore:  0.001002628191057031\n",
      "Words:  ('width', 'height') \t\tScore:  0.0009729646351085983\n",
      "Words:  ('src', 'http') \t\tScore:  0.0009314356567807923\n",
      "Words:  ('www', 'computron') \t\tScore:  0.0009017721008323594\n",
      "Words:  ('forward', 'looking') \t\tScore:  0.0008424449889354936\n",
      "Words:  ('http', 'nd') \t\tScore:  0.0008068487217973741\n",
      "Words:  ('td', 'td') \t\tScore:  0.000794983299418001\n",
      "Words:  ('investment', 'advice') \t\tScore:  0.0007000599203830158\n",
      "Words:  ('font', 'size') \t\tScore:  0.0006822617868139561\n",
      "Words:  ('td', 'tr') \t\tScore:  0.0006585309420552098\n",
      "Words:  ('tr', 'td') \t\tScore:  0.000557674851830538\n",
      "Words:  ('align', 'center') \t\tScore:  0.0005398767182614783\n",
      "Words:  ('size', 'pt') \t\tScore:  0.000516145873502732\n",
      "Words:  ('windows', 'xp') \t\tScore:  0.0005042804511233589\n",
      "Words:  ('duty', 'free') \t\tScore:  0.00048648231755429915\n",
      "Words:  ('ali', 'duty') \t\tScore:  0.0004805496063646126\n",
      "Words:  ('font', 'family') \t\tScore:  0.0004805496063646126\n",
      "Words:  ('td', 'width') \t\tScore:  0.0004805496063646126\n",
      "Words:  ('within', 'email') \t\tScore:  0.0004686841839852394\n",
      "Words:  ('risks', 'uncertainties') \t\tScore:  0.00046275147279555283\n",
      "Words:  ('xp', 'professional') \t\tScore:  0.00045681876160586627\n",
      "Words:  ('soft', 'tabs') \t\tScore:  0.0004508860504161797\n",
      "Words:  ('border', 'src') \t\tScore:  0.00043308791684711995\n",
      "Words:  ('moopid', 'hotlist') \t\tScore:  0.00043308791684711995\n",
      "Words:  ('www', 'moopid') \t\tScore:  0.00043308791684711995\n",
      "Words:  ('copy', 'paste') \t\tScore:  0.0004212224944677468\n",
      "Words:  ('height', 'td') \t\tScore:  0.0004212224944677468\n",
      "Words:  ('pt', 'font') \t\tScore:  0.0004212224944677468\n",
      "Words:  ('tr', 'tr') \t\tScore:  0.0004212224944677468\n",
      "Words:  ('adobe', 'photoshop') \t\tScore:  0.00038562622732962737\n",
      "Words:  ('jpg', 'width') \t\tScore:  0.00038562622732962737\n",
      "Words:  ('mg', 'pills') \t\tScore:  0.00038562622732962737\n",
      "Words:  ('statements', 'made') \t\tScore:  0.00038562622732962737\n",
      "Words:  ('family', 'knle') \t\tScore:  0.0003796935161399408\n",
      "Words:  ('height', 'pt') \t\tScore:  0.0003796935161399408\n",
      "Words:  ('knle', 'font') \t\tScore:  0.0003796935161399408\n",
      "Words:  ('line', 'height') \t\tScore:  0.0003796935161399408\n",
      "Words:  ('securities', 'act') \t\tScore:  0.0003796935161399408\n",
      "Words:  ('style', 'line') \t\tScore:  0.0003796935161399408\n",
      "Words:  ('best', 'regards') \t\tScore:  0.00037376080495025424\n",
      "Words:  ('iit', 'demokritos') \t\tScore:  0.0003678280937605676\n",
      "Words:  ('demokritos', 'gr') \t\tScore:  0.00036189538257088105\n",
      "Words:  ('without', 'notice') \t\tScore:  0.0003559626713811945\n",
      "Words:  ('h', 'r') \t\tScore:  0.0003500299601915079\n",
      "Words:  ('within', 'report') \t\tScore:  0.0003500299601915079\n",
      "Words:  ('money', 'back') \t\tScore:  0.00034409724900182135\n"
     ]
    }
   ],
   "source": [
    "#Create the bigram finder for SPAM#\n",
    "SPAMbcf = BigramCollocationFinder.from_words(nostopSPAMTOKENS)\n",
    "\n",
    "SPAMbcf.apply_freq_filter(4)\n",
    "SPAMbcfscores = SPAMbcf.score_ngrams(Bigrammeasures.raw_freq)\n",
    "# check the top 50 scores\n",
    "for score in SPAMbcfscores[:50]:\n",
    "    print(\"Words: \", score[0], \"\\t\\tScore: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words:  ('hou', 'ect') \t\tScore:  0.024684610207936924\n",
      "Words:  ('ect', 'ect') \t\tScore:  0.021657961350445627\n",
      "Words:  ('enron', 'enron') \t\tScore:  0.004973815729692245\n",
      "Words:  ('ect', 'cc') \t\tScore:  0.00475518646684863\n",
      "Words:  ('corp', 'enron') \t\tScore:  0.004147123829564826\n",
      "Words:  ('let', 'know') \t\tScore:  0.0033409284228289944\n",
      "Words:  ('daren', 'j') \t\tScore:  0.003276022860422296\n",
      "Words:  ('j', 'farmer') \t\tScore:  0.0027875231012560934\n",
      "Words:  ('ect', 'pm') \t\tScore:  0.0019369186255051532\n",
      "Words:  ('farmer', 'hou') \t\tScore:  0.0019232542965774272\n",
      "Words:  ('attached', 'file') \t\tScore:  0.0017148732804296064\n",
      "Words:  ('enron', 'cc') \t\tScore:  0.0016875446225741546\n",
      "Words:  ('enron', 'hpl') \t\tScore:  0.00142450629071543\n",
      "Words:  ('pec', 'pec') \t\tScore:  0.0012195413567995409\n",
      "Words:  ('teco', 'tap') \t\tScore:  0.0011375553832331851\n",
      "Words:  ('ami', 'chokshi') \t\tScore:  0.001052153327434898\n",
      "Words:  ('north', 'america') \t\tScore:  0.0010316568340433092\n",
      "Words:  ('tenaska', 'iv') \t\tScore:  0.0010214085873475147\n",
      "Words:  ('enron', 'north') \t\tScore:  0.0009838316827962682\n",
      "Words:  ('robert', 'cotten') \t\tScore:  0.0009735834361004739\n",
      "Words:  ('vance', 'taylor') \t\tScore:  0.0009633351894046794\n",
      "Words:  ('pat', 'clynes') \t\tScore:  0.0009155100381576385\n",
      "Words:  ('enron', 'pm') \t\tScore:  0.0009120939559257071\n",
      "Words:  ('robert', 'lloyd') \t\tScore:  0.0008984296269979811\n",
      "Words:  ('america', 'corp') \t\tScore:  0.0008711009691425292\n",
      "Words:  ('melissa', 'graves') \t\tScore:  0.0008608527224467348\n",
      "Words:  ('cotten', 'hou') \t\tScore:  0.0008506044757509403\n",
      "Words:  ('na', 'enron') \t\tScore:  0.0008471883935190087\n",
      "Words:  ('hpl', 'nom') \t\tScore:  0.0008437723112870773\n",
      "Words:  ('clynes', 'corp') \t\tScore:  0.000792531077808105\n",
      "Words:  ('hplno', 'xls') \t\tScore:  0.0007652024199526531\n",
      "Words:  ('ect', 'enron') \t\tScore:  0.0007412898443291327\n",
      "Words:  ('gary', 'hanks') \t\tScore:  0.0007344576798652698\n",
      "Words:  ('julie', 'meyers') \t\tScore:  0.0007344576798652698\n",
      "Words:  ('chokshi', 'corp') \t\tScore:  0.0007207933509375437\n",
      "Words:  ('tap', 'enron') \t\tScore:  0.0007207933509375437\n",
      "Words:  ('aimee', 'lannou') \t\tScore:  0.0007173772687056123\n",
      "Words:  ('smith', 'hou') \t\tScore:  0.0007173772687056123\n",
      "Words:  ('texas', 'utilities') \t\tScore:  0.0007002968575459548\n",
      "Words:  ('pefs', 'pec') \t\tScore:  0.0006763842819224344\n",
      "Words:  ('enron', 'enronxgate') \t\tScore:  0.000672968199690503\n",
      "Words:  ('graves', 'hou') \t\tScore:  0.0006695521174585714\n",
      "Words:  ('cec', 'pec') \t\tScore:  0.00066613603522664\n",
      "Words:  ('gcs', 'cec') \t\tScore:  0.00066613603522664\n",
      "Words:  ('jackie', 'young') \t\tScore:  0.0006524717062989141\n",
      "Words:  ('hpl', 'actuals') \t\tScore:  0.0006490556240669825\n",
      "Words:  ('hplo', 'xls') \t\tScore:  0.0006490556240669825\n",
      "Words:  ('mmbtu', 'hsc') \t\tScore:  0.0006285591306753936\n",
      "Words:  ('lloyd', 'hou') \t\tScore:  0.0006183108839795991\n",
      "Words:  ('george', 'weissman') \t\tScore:  0.0006046465550518732\n"
     ]
    }
   ],
   "source": [
    "#Create the bigram finder for HAM#\n",
    "HAMbcf = BigramCollocationFinder.from_words(nostopHAMTOKENS)\n",
    "\n",
    "HAMbcf.apply_freq_filter(4)\n",
    "HAMbcfscores = HAMbcf.score_ngrams(Bigrammeasures.raw_freq)\n",
    "# check the top 50 scores\n",
    "for score in HAMbcfscores[:50]:\n",
    "    print(\"Words: \", score[0], \"\\t\\tScore: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Trigram Association Measure#\n",
    "Trigrammeasures = nltk.collocations.TrigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words:  ('nbsp', 'nbsp', 'nbsp') \t\tScore:  0.001453514241473211\n",
      "Words:  ('forward', 'looking', 'statements') \t\tScore:  0.0008246468553664339\n",
      "Words:  ('pills', 'pills', 'pills') \t\tScore:  0.0005398767182614783\n",
      "Words:  ('font', 'size', 'pt') \t\tScore:  0.000516145873502732\n",
      "Words:  ('src', 'http', 'nd') \t\tScore:  0.0004983477399336723\n",
      "Words:  ('ali', 'duty', 'free') \t\tScore:  0.0004805496063646126\n",
      "Words:  ('href', 'http', 'www') \t\tScore:  0.0004508860504161797\n",
      "Words:  ('border', 'src', 'http') \t\tScore:  0.00043308791684711995\n",
      "Words:  ('http', 'www', 'moopid') \t\tScore:  0.00043308791684711995\n",
      "Words:  ('www', 'moopid', 'hotlist') \t\tScore:  0.00043308791684711995\n",
      "Words:  ('td', 'tr', 'tr') \t\tScore:  0.0004212224944677468\n",
      "Words:  ('width', 'height', 'td') \t\tScore:  0.0004212224944677468\n",
      "Words:  ('td', 'td', 'width') \t\tScore:  0.0004093570720883737\n",
      "Words:  ('pt', 'font', 'family') \t\tScore:  0.00039155893851931393\n",
      "Words:  ('family', 'knle', 'font') \t\tScore:  0.0003796935161399408\n",
      "Words:  ('font', 'family', 'knle') \t\tScore:  0.0003796935161399408\n",
      "Words:  ('height', 'pt', 'font') \t\tScore:  0.0003796935161399408\n",
      "Words:  ('knle', 'font', 'size') \t\tScore:  0.0003796935161399408\n",
      "Words:  ('line', 'height', 'pt') \t\tScore:  0.0003796935161399408\n",
      "Words:  ('style', 'line', 'height') \t\tScore:  0.0003796935161399408\n",
      "Words:  ('tr', 'tr', 'td') \t\tScore:  0.0003796935161399408\n",
      "Words:  ('mg', 'pills', 'pills') \t\tScore:  0.00037376080495025424\n",
      "Words:  ('src', 'http', 'www') \t\tScore:  0.00037376080495025424\n",
      "Words:  ('iit', 'demokritos', 'gr') \t\tScore:  0.00036189538257088105\n",
      "Words:  ('jebel', 'ali', 'duty') \t\tScore:  0.0003262991154327616\n",
      "Words:  ('statements', 'within', 'meaning') \t\tScore:  0.00032036640424307504\n",
      "Words:  ('phone', 'mobile', 'email') \t\tScore:  0.00030256827067401534\n",
      "Words:  ('href', 'http', 'nd') \t\tScore:  0.0002966355594843287\n",
      "Words:  ('http', 'nd', 'cf') \t\tScore:  0.00029070284829464215\n",
      "Words:  ('http', 'nd', 'ftar') \t\tScore:  0.00027290471472558246\n",
      "Words:  ('td', 'width', 'height') \t\tScore:  0.00027290471472558246\n",
      "Words:  ('gif', 'width', 'height') \t\tScore:  0.0002669720035358959\n",
      "Words:  ('height', 'href', 'http') \t\tScore:  0.0002669720035358959\n",
      "Words:  ('inherent', 'conflict', 'interest') \t\tScore:  0.00026103929234620927\n",
      "Words:  ('visit', 'http', 'www') \t\tScore:  0.00026103929234620927\n",
      "Words:  ('windows', 'xp', 'professional') \t\tScore:  0.00026103929234620927\n",
      "Words:  ('moopid', 'hotlist', 'images') \t\tScore:  0.0002551065811565227\n",
      "Words:  ('plain', 'text', 'format') \t\tScore:  0.0002551065811565227\n",
      "Words:  ('logos', 'trademarks', 'property') \t\tScore:  0.00024917386996683614\n",
      "Words:  ('looking', 'statements', 'within') \t\tScore:  0.00024917386996683614\n",
      "Words:  ('risks', 'uncertainties', 'cause') \t\tScore:  0.00024917386996683614\n",
      "Words:  ('trademarks', 'property', 'respective') \t\tScore:  0.00024917386996683614\n",
      "Words:  ('bill', 'title', 'iii') \t\tScore:  0.00024324115877714957\n",
      "Words:  ('click', 'link', 'copy') \t\tScore:  0.00024324115877714957\n",
      "Words:  ('commercial', 'h', 'r') \t\tScore:  0.00024324115877714957\n",
      "Words:  ('compliance', 'federal', 'legislation') \t\tScore:  0.00024324115877714957\n",
      "Words:  ('computron', 'follow', 'link') \t\tScore:  0.00024324115877714957\n",
      "Words:  ('congress', 'logos', 'trademarks') \t\tScore:  0.00024324115877714957\n",
      "Words:  ('considered', 'spam', 'long') \t\tScore:  0.00024324115877714957\n",
      "Words:  ('contact', 'remove', 'instructions') \t\tScore:  0.00024324115877714957\n"
     ]
    }
   ],
   "source": [
    "#Create the Trigram finder for SPAM#\n",
    "SPAMtcf = TrigramCollocationFinder.from_words(nostopSPAMTOKENS)\n",
    "\n",
    "SPAMtcf.apply_freq_filter(4)\n",
    "SPAMtcfscores = SPAMtcf.score_ngrams(Trigrammeasures.raw_freq)\n",
    "# check the top 50 scores\n",
    "for score in SPAMtcfscores[:50]:\n",
    "    print(\"Words: \", score[0], \"\\t\\tScore: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words:  ('hou', 'ect', 'ect') \t\tScore:  0.021422251676442357\n",
      "Words:  ('ect', 'ect', 'cc') \t\tScore:  0.004724441726761247\n",
      "Words:  ('corp', 'enron', 'enron') \t\tScore:  0.0031188830777534476\n",
      "Words:  ('daren', 'j', 'farmer') \t\tScore:  0.0027875231012560934\n",
      "Words:  ('farmer', 'hou', 'ect') \t\tScore:  0.0019232542965774272\n",
      "Words:  ('j', 'farmer', 'hou') \t\tScore:  0.0019232542965774272\n",
      "Words:  ('hou', 'ect', 'pm') \t\tScore:  0.0015884782378481415\n",
      "Words:  ('enron', 'enron', 'cc') \t\tScore:  0.0010419050807391036\n",
      "Words:  ('enron', 'north', 'america') \t\tScore:  0.0009667512716366108\n",
      "Words:  ('north', 'america', 'corp') \t\tScore:  0.0008711009691425292\n",
      "Words:  ('cotten', 'hou', 'ect') \t\tScore:  0.0008506044757509403\n",
      "Words:  ('robert', 'cotten', 'hou') \t\tScore:  0.0008506044757509403\n",
      "Words:  ('clynes', 'corp', 'enron') \t\tScore:  0.000792531077808105\n",
      "Words:  ('pat', 'clynes', 'corp') \t\tScore:  0.000792531077808105\n",
      "Words:  ('ami', 'chokshi', 'corp') \t\tScore:  0.0007207933509375437\n",
      "Words:  ('chokshi', 'corp', 'enron') \t\tScore:  0.0007207933509375437\n",
      "Words:  ('smith', 'hou', 'ect') \t\tScore:  0.0007173772687056123\n",
      "Words:  ('teco', 'tap', 'enron') \t\tScore:  0.0007173772687056123\n",
      "Words:  ('na', 'enron', 'enron') \t\tScore:  0.0007139611864736808\n",
      "Words:  ('graves', 'hou', 'ect') \t\tScore:  0.0006695521174585714\n",
      "Words:  ('melissa', 'graves', 'hou') \t\tScore:  0.0006695521174585714\n",
      "Words:  ('gcs', 'cec', 'pec') \t\tScore:  0.00066613603522664\n",
      "Words:  ('cec', 'pec', 'pec') \t\tScore:  0.0006490556240669825\n",
      "Words:  ('enron', 'hpl', 'actuals') \t\tScore:  0.000645639541835051\n",
      "Words:  ('lloyd', 'hou', 'ect') \t\tScore:  0.0006183108839795991\n",
      "Words:  ('robert', 'lloyd', 'hou') \t\tScore:  0.0006183108839795991\n",
      "Words:  ('tap', 'enron', 'hpl') \t\tScore:  0.0006080626372838046\n",
      "Words:  ('texas', 'utilities', 'tu') \t\tScore:  0.0006012304728199417\n",
      "Words:  ('howard', 'b', 'camp') \t\tScore:  0.0005773178971964214\n",
      "Words:  ('pefs', 'pec', 'pec') \t\tScore:  0.0005636535682686953\n",
      "Words:  ('julie', 'meyers', 'hou') \t\tScore:  0.0005499892393409694\n",
      "Words:  ('meyers', 'hou', 'ect') \t\tScore:  0.0005499892393409694\n",
      "Words:  ('hou', 'ect', 'enron') \t\tScore:  0.000546573157109038\n",
      "Words:  ('ect', 'daren', 'j') \t\tScore:  0.0005431570748771064\n",
      "Words:  ('ect', 'pat', 'clynes') \t\tScore:  0.0005431570748771064\n",
      "Words:  ('ect', 'ect', 'pat') \t\tScore:  0.000539740992645175\n",
      "Words:  ('taylor', 'hou', 'ect') \t\tScore:  0.000532908828181312\n",
      "Words:  ('b', 'camp', 'hou') \t\tScore:  0.0005294927459493805\n",
      "Words:  ('camp', 'hou', 'ect') \t\tScore:  0.0005294927459493805\n",
      "Words:  ('forwarded', 'ami', 'chokshi') \t\tScore:  0.0005226605814855175\n",
      "Words:  ('let', 'know', 'questions') \t\tScore:  0.000519244499253586\n",
      "Words:  ('vance', 'taylor', 'hou') \t\tScore:  0.000519244499253586\n",
      "Words:  ('gco', 'enron', 'enron') \t\tScore:  0.0005089962525577916\n",
      "Words:  ('aimee', 'lannou', 'hou') \t\tScore:  0.0004919158413981341\n",
      "Words:  ('lannou', 'hou', 'ect') \t\tScore:  0.0004919158413981341\n",
      "Words:  ('farmer', 'daren', 'j') \t\tScore:  0.0004884997591662027\n",
      "Words:  ('ect', 'ect', 'robert') \t\tScore:  0.00048508367693427115\n",
      "Words:  ('pm', 'daren', 'j') \t\tScore:  0.00048508367693427115\n",
      "Words:  ('fuels', 'cotton', 'valley') \t\tScore:  0.0004782515124704082\n",
      "Words:  ('ect', 'ect', 'daren') \t\tScore:  0.0004645871835426822\n"
     ]
    }
   ],
   "source": [
    "#Create the Trigram finder for HAM#\n",
    "HAMtcf = TrigramCollocationFinder.from_words(nostopHAMTOKENS)\n",
    "\n",
    "HAMtcf.apply_freq_filter(4)\n",
    "HAMtcfscores = HAMtcf.score_ngrams(Trigrammeasures.raw_freq)\n",
    "# check the top 50 scores\n",
    "for score in HAMtcfscores[:50]:\n",
    "    print(\"Words: \", score[0], \"\\t\\tScore: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('ttfkevh', 'vldeo'), 15.362877016654133)\n",
      "(('xxca', 'vldeo'), 15.362877016654133)\n",
      "(('ansi', 'lumens'), 14.94783951737529)\n",
      "(('brightness', 'ansi'), 14.94783951737529)\n",
      "(('dlp', 'projectors'), 14.94783951737529)\n",
      "(('eres', 'charterpipeline'), 14.94783951737529)\n",
      "(('projectors', 'tdp'), 14.94783951737529)\n",
      "(('taciturnly', 'ello'), 14.94783951737529)\n",
      "(('vesicopubic', 'ello'), 14.94783951737529)\n",
      "(('abdul', 'kareem'), 14.362877016654133)\n",
      "(('gwmvurvtdu', 'dreamer'), 14.362877016654133)\n",
      "(('hollerin', 'chu'), 14.362877016654133)\n",
      "(('psc', 'scanner'), 14.362877016654133)\n",
      "(('shut', 'kaleste'), 14.362877016654133)\n",
      "(('siberiantigergrowl', 'hotmail'), 14.362877016654133)\n",
      "(('cadeau', 'spcial'), 14.362877016654132)\n",
      "(('shahalam', 'selangord'), 14.362877016654132)\n",
      "(('incest', 'samples'), 14.140484595317686)\n",
      "(('alleviates', 'mild'), 14.04094892176677)\n",
      "(('bottles', 'valarx'), 13.94783951737529)\n",
      "(('erp', 'erp'), 13.94783951737529)\n",
      "(('guadalajara', 'mexico'), 13.94783951737529)\n",
      "(('lagoon', 'backyardkatie'), 13.94783951737529)\n",
      "(('mexico', 'monterrey'), 13.94783951737529)\n",
      "(('sehr', 'geehrter'), 13.94783951737529)\n",
      "(('agrra', 'adlpex'), 13.777914515932977)\n",
      "(('aliuum', 'ciaaliis'), 13.777914515932977)\n",
      "(('alr', 'gigccb'), 13.777914515932977)\n",
      "(('ambieen', 'medssno'), 13.777914515932977)\n",
      "(('ambl', 'tussioneex'), 13.777914515932977)\n",
      "(('baune', 'ebel'), 13.777914515932977)\n",
      "(('baune', 'mercier'), 13.777914515932977)\n",
      "(('beneficl', 'qslw'), 13.777914515932977)\n",
      "(('commercialize', 'scaie'), 13.777914515932977)\n",
      "(('compet', 'itive'), 13.777914515932977)\n",
      "(('compet', 'popularly'), 13.777914515932977)\n",
      "(('crk', 'wsllhk'), 13.777914515932977)\n",
      "(('defendant', 'lawsuits'), 13.777914515932977)\n",
      "(('discreeeeet', 'nauauauaghty'), 13.777914515932977)\n",
      "(('dissolvable', 'absorbed'), 13.777914515932977)\n",
      "(('dkny', 'baune'), 13.777914515932977)\n",
      "(('dkny', 'ebel'), 13.777914515932977)\n",
      "(('dkny', 'mercier'), 13.777914515932977)\n",
      "(('elr', 'tsx'), 13.777914515932977)\n",
      "(('erlc', 'vlag'), 13.777914515932977)\n",
      "(('everyy', 'lowerr'), 13.777914515932977)\n",
      "(('everyy', 'offf'), 13.777914515932977)\n",
      "(('everyy', 'wontt'), 13.777914515932977)\n",
      "(('fontbrbr', 'bretail'), 13.777914515932977)\n",
      "(('fsb', 'fdic'), 13.777914515932977)\n"
     ]
    }
   ],
   "source": [
    "#Create PMI for SPAM (Bigrams)#\n",
    "SPAMpmibcf = BigramCollocationFinder.from_words(nostopSPAMTOKENS,window_size = 5)\n",
    "\n",
    "SPAMpmibcf.apply_freq_filter(3)\n",
    "SPAMpmiscores = SPAMpmibcf.score_ngrams(Bigrammeasures.pmi)\n",
    "\n",
    "for item in SPAMpmiscores[:50]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('ffot', 'tatum'), 16.159225865392745)\n",
      "(('cuba', 'libre'), 15.744188366113903)\n",
      "(('electronics', 'recreation'), 15.744188366113903)\n",
      "(('hemmings', 'badham'), 15.744188366113903)\n",
      "(('housewares', 'watches'), 15.744188366113903)\n",
      "(('ochs', 'bros'), 15.744188366113903)\n",
      "(('sanger', 'heirs'), 15.744188366113903)\n",
      "(('appliances', 'electronics'), 15.574263364671593)\n",
      "(('electronics', 'fitness'), 15.574263364671593)\n",
      "(('executable', 'arserver'), 15.159225865392749)\n",
      "(('housewares', 'jewelry'), 15.159225865392749)\n",
      "(('odum', 'luminant'), 15.159225865392749)\n",
      "(('samir', 'salama'), 15.159225865392749)\n",
      "(('stern', 'nyu'), 15.159225865392749)\n",
      "(('unwind', 'hedges'), 15.159225865392749)\n",
      "(('capitol', 'switchboard'), 15.159225865392747)\n",
      "(('fitness', 'recreation'), 15.159225865392747)\n",
      "(('frens', 'mpg'), 15.159225865392747)\n",
      "(('groetzinger', 'peusa'), 15.159225865392747)\n",
      "(('hunting', 'mpe'), 15.159225865392747)\n",
      "(('imai', 'rika'), 15.159225865392747)\n",
      "(('ssl', 'jreveffo'), 15.159225865392747)\n",
      "(('tatum', 'br'), 15.159225865392747)\n",
      "(('nomform', 'wk'), 14.989300863950437)\n",
      "(('pgordon', 'sfasu'), 14.989300863950437)\n",
      "(('deinetol', 'dukepower'), 14.896191459558954)\n",
      "(('alvestad', 'dheineke'), 14.837297770505387)\n",
      "(('alvestad', 'tsteel'), 14.837297770505387)\n",
      "(('credits', 'annually'), 14.837297770505387)\n",
      "(('baer', 'middlebury'), 14.744188366113903)\n",
      "(('howry', 'lvan'), 14.744188366113903)\n",
      "(('jewelry', 'watches'), 14.744188366113903)\n",
      "(('neals', 'hump'), 14.744188366113903)\n",
      "(('tcash', 'aimtech'), 14.744188366113903)\n",
      "(('rename', 'arserver'), 14.574263364671593)\n",
      "(('abdul', 'raheem'), 14.574263364671591)\n",
      "(('adrial', 'boals'), 14.574263364671591)\n",
      "(('airam', 'arteaga'), 14.574263364671591)\n",
      "(('amita', 'gosalia'), 14.574263364671591)\n",
      "(('armiger', 'armiger'), 14.574263364671591)\n",
      "(('ashamed', 'ashamed'), 14.574263364671591)\n",
      "(('casey', 'dahlke'), 14.574263364671591)\n",
      "(('cgi', 'bin'), 14.574263364671591)\n",
      "(('des', 'moines'), 14.574263364671591)\n",
      "(('duck', 'hunting'), 14.574263364671591)\n",
      "(('fig', 'orchard'), 14.574263364671591)\n",
      "(('ho', 'ho'), 14.574263364671591)\n",
      "(('houstonexp', 'cnrl'), 14.574263364671591)\n",
      "(('houstonexp', 'patb'), 14.574263364671591)\n",
      "(('hurdle', 'securing'), 14.574263364671591)\n"
     ]
    }
   ],
   "source": [
    "#Create PMI for HAM (Bigrams)#\n",
    "HAMpmibcf = BigramCollocationFinder.from_words(nostopHAMTOKENS,window_size = 5)\n",
    "\n",
    "HAMpmibcf.apply_freq_filter(3)\n",
    "HAMpmiscores = HAMpmibcf.score_ngrams(Bigrammeasures.pmi)\n",
    "\n",
    "for item in HAMpmiscores[:50]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('xxca', 'ttfkevh', 'vldeo'), 32.14079153258711)\n",
      "(('vesicopubic', 'taciturnly', 'ello'), 31.725754033308263)\n",
      "(('ttfkevh', 'vldeo', 'vldeo'), 30.55582903186595)\n",
      "(('xxca', 'vldeo', 'vldeo'), 30.55582903186595)\n",
      "(('taciturnly', 'ello', 'ello'), 29.725754033308263)\n",
      "(('vesicopubic', 'ello', 'ello'), 29.725754033308263)\n",
      "(('shahalam', 'selangord', 'malaysia'), 29.555829031865954)\n",
      "(('gwmvurvtdu', 'dreamer', 'tw'), 29.14079153258711)\n",
      "(('baune', 'mercier', 'ebel'), 28.970866531144793)\n",
      "(('compet', 'itive', 'popularly'), 28.970866531144793)\n",
      "(('dkny', 'baune', 'ebel'), 28.970866531144793)\n",
      "(('dkny', 'baune', 'mercier'), 28.970866531144793)\n",
      "(('dkny', 'mercier', 'ebel'), 28.970866531144793)\n",
      "(('everyy', 'offf', 'lowerr'), 28.970866531144793)\n",
      "(('everyy', 'offf', 'wontt'), 28.970866531144793)\n",
      "(('everyy', 'wontt', 'lowerr'), 28.970866531144793)\n",
      "(('hoooooooooook', 'discreeeeet', 'nauauauaghty'), 28.970866531144793)\n",
      "(('kunstmarkt', 'infos', 'thierryehrmann'), 28.970866531144793)\n",
      "(('lowesst', 'everyy', 'offf'), 28.970866531144793)\n",
      "(('mediccationns', 'everyy', 'offf'), 28.970866531144793)\n",
      "(('mediccationns', 'lowesst', 'everyy'), 28.970866531144793)\n",
      "(('mediccationns', 'lowesst', 'offf'), 28.970866531144793)\n",
      "(('occurrences', 'optics', 'conductors'), 28.970866531144793)\n",
      "(('occurrences', 'optics', 'semi'), 28.970866531144793)\n",
      "(('occurrences', 'semi', 'conductors'), 28.970866531144793)\n",
      "(('offf', 'lowerr', 'selll'), 28.970866531144793)\n",
      "(('offf', 'wontt', 'lowerr'), 28.970866531144793)\n",
      "(('offf', 'wontt', 'selll'), 28.970866531144793)\n",
      "(('optics', 'semi', 'conductors'), 28.970866531144793)\n",
      "(('physics', 'occurrences', 'optics'), 28.970866531144793)\n",
      "(('vgr', 'vlm', 'xnx'), 28.970866531144793)\n",
      "(('welt', 'infos', 'thierryehrmann'), 28.970866531144793)\n",
      "(('welt', 'kunstmarkt', 'infos'), 28.970866531144793)\n",
      "(('welt', 'kunstmarkt', 'thierryehrmann'), 28.970866531144793)\n",
      "(('wontt', 'lowerr', 'selll'), 28.970866531144793)\n",
      "(('xom', 'wmb', 'wgr'), 28.970866531144793)\n",
      "(('aliuum', 'ambiien', 'ciaaliis'), 28.555829031865954)\n",
      "(('baune', 'ebel', 'corum'), 28.555829031865954)\n",
      "(('baune', 'mercier', 'corum'), 28.555829031865954)\n",
      "(('crk', 'uo', 'wsllhk'), 28.555829031865954)\n",
      "(('dudes', 'hoooooooooook', 'discreeeeet'), 28.555829031865954)\n",
      "(('fendi', 'baune', 'mercier'), 28.555829031865954)\n",
      "(('fendi', 'dkny', 'baune'), 28.555829031865954)\n",
      "(('fendi', 'dkny', 'mercier'), 28.555829031865954)\n",
      "(('fsb', 'fdic', 'arc'), 28.555829031865954)\n",
      "(('graa', 'cai', 'llis'), 28.555829031865954)\n",
      "(('lec', 'girard', 'titoni'), 28.555829031865954)\n",
      "(('lowerr', 'selll', 'od'), 28.555829031865954)\n",
      "(('lumens', 'contrast', 'ratio'), 28.555829031865954)\n",
      "(('meeeeeeeeeeet', 'dudes', 'hoooooooooook'), 28.555829031865954)\n"
     ]
    }
   ],
   "source": [
    "#Create PMI for SPAM (Trigrams)#\n",
    "SPAMpmitcf = TrigramCollocationFinder.from_words(nostopSPAMTOKENS, window_size = 5)\n",
    "\n",
    "SPAMpmitcf.apply_freq_filter(3)\n",
    "SPAMpmitscores = SPAMpmitcf.score_ngrams(Trigrammeasures.pmi)\n",
    "SPAMpmit = SPAMpmitcf.nbest(Trigrammeasures.pmi,50)\n",
    "\n",
    "for item in SPAMpmitscores[:50]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('ffot', 'tatum', 'tatum'), 32.14852672934319)\n",
      "(('electronics', 'fitness', 'recreation'), 32.14852672934318)\n",
      "(('deinetol', 'dukepower', 'mxsdo'), 31.733489230064343)\n",
      "(('housewares', 'jewelry', 'watches'), 31.733489230064336)\n",
      "(('appliances', 'electronics', 'fitness'), 31.563564228622027)\n",
      "(('nomform', 'wk', 'kaf'), 31.14852672934319)\n",
      "(('tatum', 'tatum', 'br'), 31.14852672934319)\n",
      "(('executable', 'rename', 'arserver'), 31.148526729343182)\n",
      "(('alvestad', 'dheineke', 'tsteel'), 30.674595541010767)\n",
      "(('houstonexp', 'patb', 'cnrl'), 30.563564228622027)\n",
      "(('ilene', 'erskine', 'azurix'), 30.563564228622027)\n",
      "(('llipperdt', 'houstonexp', 'cnrl'), 30.563564228622027)\n",
      "(('llipperdt', 'houstonexp', 'patb'), 30.563564228622027)\n",
      "(('llipperdt', 'patb', 'cnrl'), 30.563564228622027)\n",
      "(('magazine', 'kits', 'jigs'), 30.563564228622027)\n",
      "(('shopnotes', 'magazine', 'kits'), 30.563564228622027)\n",
      "(('spinexp', 'houstonexp', 'patb'), 30.563564228622027)\n",
      "(('spinexp', 'llipperdt', 'houstonexp'), 30.563564228622027)\n",
      "(('spinexp', 'llipperdt', 'patb'), 30.563564228622027)\n",
      "(('stretched', 'thin', 'magnitude'), 30.563564228622027)\n",
      "(('tjones', 'llipperdt', 'houstonexp'), 30.563564228622027)\n",
      "(('tjones', 'spinexp', 'houstonexp'), 30.563564228622027)\n",
      "(('tjones', 'spinexp', 'llipperdt'), 30.563564228622027)\n",
      "(('guards', 'idiot', 'proofs'), 30.14852672934319)\n",
      "(('emission', 'credits', 'annually'), 30.089633040289613)\n",
      "(('christian', 'ffot', 'tatum'), 30.033049511923252)\n",
      "(('woodsmith', 'magazine', 'workbench'), 29.978601727900866)\n",
      "(('laurel', 'mallak', 'mutaz'), 29.826598634455824)\n",
      "(('neals', 'hump', 'pgordon'), 29.733489230064343)\n",
      "(('rel', 'loc', 'density'), 29.733489230064336)\n",
      "(('abdul', 'raheem', 'herman'), 29.563564228622027)\n",
      "(('nachlinger', 'abdul', 'raheem'), 29.563564228622027)\n",
      "(('pete', 'groetzinger', 'peusa'), 29.563564228622027)\n",
      "(('sengupta', 'jayanta', 'melethil'), 29.563564228622027)\n",
      "(('wiggins', 'fig', 'orchard'), 29.563564228622027)\n",
      "(('reg', 'preamble', 'fercsr'), 29.41156113517698)\n",
      "(('accountonline', 'cb', 'dcl'), 29.411561135176974)\n",
      "(('barring', 'unseen', 'mg'), 29.411561135176974)\n",
      "(('seahawk', 'mat', 'plt'), 29.411561135176974)\n",
      "(('dheineke', 'tsteel', 'heineke'), 29.352667446123405)\n",
      "(('retreat', 'ho', 'ho'), 29.341171807285583)\n",
      "(('docket', 'rm', 'fr'), 29.241636133734662)\n",
      "(('cough', 'elixir', 'dimetapp'), 29.18916871384053)\n",
      "(('brand', 'battery', 'runs'), 29.148526729343182)\n",
      "(('https', 'accountonline', 'cb'), 29.148526729343182)\n",
      "(('rename', 'arserver', 'exe'), 29.148526729343182)\n",
      "(('pvr', 'avail', 'dbq'), 29.089633040289613)\n",
      "(('grndsnla', 'jpg', 'grandsn'), 29.070524217341912)\n",
      "(('ho', 'ho', 'wonderful'), 28.978601727900873)\n",
      "(('prompting', 'abort', 'signer'), 28.978601727900873)\n"
     ]
    }
   ],
   "source": [
    "#Create PMI for HAM (Trigrams)#\n",
    "HAMpmitcf = TrigramCollocationFinder.from_words(nostopHAMTOKENS, window_size = 5)\n",
    "\n",
    "HAMpmitcf.apply_freq_filter(3)\n",
    "HAMpmitscores = HAMpmitcf.score_ngrams(Trigrammeasures.pmi)\n",
    "HAMpmit = HAMpmitcf.nbest(Trigrammeasures.pmi,50)\n",
    "\n",
    "for item in HAMpmitscores[:50]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham', 'ham', 'spam', 'spam', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'spam', 'spam', 'spam', 'ham', 'spam', 'ham', 'spam', 'spam', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'spam', 'ham', 'spam', 'ham', 'spam']\n"
     ]
    }
   ],
   "source": [
    "random.seed(188)\n",
    "random.shuffle(emaildocs)\n",
    "print([doc[1] for doc in emaildocs[:50]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print POS Tag frequencies\n",
    "def getPosStats(emaildocs):\n",
    "    DOC = emaildocs\n",
    "    # get list of tags\n",
    "    POS = [t[1] for t in nltk.pos_tag(DOC)]\n",
    "    # aggregate class counts\n",
    "    NounCount, VerbCount, AdjCount, AdvCount = 0, 0, 0, 0\n",
    "    for tag in POS:\n",
    "        if tag.startswith('N'): NounCount += 1\n",
    "        if tag.startswith('V'): VerbCount += 1\n",
    "        if tag.startswith('J'): AdjCount += 1\n",
    "        if tag.startswith('R'): AdvCount += 1\n",
    "    # normalize class counts\n",
    "    NormNounCount = NounCount / len(POS); print(\"Noun Frequency: {:.2f}\".format(NormNounCount))\n",
    "    NormVerbCount = VerbCount / len(POS); print(\"Verb Frequency: {:.2f}\".format(NormVerbCount))\n",
    "    NormAdjCount = AdjCount / len(POS); print(\"Adjective Frequency: {:.2f}\".format(NormAdjCount))\n",
    "    NormAdvCount = AdvCount / len(POS); print(\"Adverb Frequency: {:.2f}\".format(NormAdvCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun Frequency: 0.58\n",
      "Verb Frequency: 0.15\n",
      "Adjective Frequency: 0.20\n",
      "Adverb Frequency: 0.03\n"
     ]
    }
   ],
   "source": [
    "getPosStats(nostopSPAMTOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun Frequency: 0.58\n",
      "Verb Frequency: 0.19\n",
      "Adjective Frequency: 0.18\n",
      "Adverb Frequency: 0.03\n"
     ]
    }
   ],
   "source": [
    "getPosStats(nostopHAMTOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gwFEAT(emaildocs, n):\n",
    "    AWL = []\n",
    "    for email in emaildocs:\n",
    "        AWL.extend(email[0])\n",
    "    AW = nltk.FreqDist(AWL)\n",
    "    wordITEM = AW.most_common(n)\n",
    "    wordFEAT = [word for (word, count) in wordITEM]\n",
    "    return wordFEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram Bag of Words\n",
    "def gbREP(email, wordFEAT, use_bool=True):\n",
    "    emailwords = email\n",
    "    features = {}\n",
    "    for word in wordFEAT:\n",
    "        if use_bool:\n",
    "            features[word] = (word in set(emailwords))\n",
    "        else:\n",
    "            features[word] = emailwords.count(word)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CVevalMET(num_folds, featuresets):\n",
    "    accuracyLIST = []\n",
    "    reference = []\n",
    "    hypothesis = []\n",
    "    subsetSIZE = int(len(featuresets) / 10)\n",
    "    for i in range(num_folds):\n",
    "        roundTEST = featuresets[(i * subsetSIZE):][:subsetSIZE]\n",
    "        roundTRAIN = featuresets[:(i * subsetSIZE)] + featuresets[((i + 1) * subsetSIZE):]\n",
    "        clf = nltk.NaiveBayesClassifier.train(roundTRAIN)\n",
    "        correct = 0\n",
    "        for case in roundTEST:\n",
    "            features = case[0]\n",
    "            label = case[1]; reference.append(label)\n",
    "            pred = clf.classify(features); hypothesis.append(pred)\n",
    "            if label == pred:\n",
    "                correct += 1\n",
    "        accuracy = float(correct) / float(len(roundTEST))\n",
    "        accuracyLIST.append(accuracy)\n",
    "        print(\"Accuracy for Fold {}: {}\".format(i, accuracy))\n",
    "    print(\"---Average Accuracy: {}---\".format(mean(accuracyLIST)))\n",
    "    labels = set(reference)\n",
    "    recallLIST = []\n",
    "    precisionLIST = []\n",
    "    f1LIST = []\n",
    "    unable_to_calc = False\n",
    "    for label in labels:\n",
    "        TP = FP = FN = TN = 0\n",
    "        for i, val in enumerate(reference):\n",
    "            if val == label and hypothesis[i] == label: TP += 1\n",
    "            elif val == label and hypothesis[i] != label: FN += 1\n",
    "            elif val != label and hypothesis[i] == label: FP += 1\n",
    "            else: TN += 1\n",
    "        try:\n",
    "            recall = TP / (TP + FP)\n",
    "            precision = TP / (TP + FN)\n",
    "            recallLIST.append(recall)\n",
    "            precisionLIST.append(precision)\n",
    "            f1LIST.append(2 * (recall * precision) / (recall + precision))\n",
    "        except ZeroDivisionError:\n",
    "            unable_to_calc = True\n",
    "    print(\"\\n\\tPrecision\\tRecall\\t\\tF1\")\n",
    "    if not unable_to_calc:\n",
    "        for i, label in enumerate(labels):\n",
    "            print(\"{}\\t{}\\t\\t{}\\t\\t{}\".format(label, round(precisionLIST[i], 3),\n",
    "                                              round(recallLIST[i], 3), round(f1LIST[i], 3)))\n",
    "    print(\"\\n\")\n",
    "    print(nltk.ConfusionMatrix(reference, hypothesis).pretty_format(sort_by_count=True))\n",
    "    print(nltk.ConfusionMatrix(reference, hypothesis).pretty_format(sort_by_count=True, show_percents=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Word Featuresets\n",
    "def getFeatureSets1(emaildocs, stopwords):\n",
    "    #get word features based on specified stopwords\n",
    "    word_features = getWordFeatures(emaildocs, stopwords)\n",
    "    featuresets = [(document_features(d, word_features), c) for (d, c) in emaildocs]\n",
    "    return featuresets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Fold 0: 0.9458413926499033\n",
      "Accuracy for Fold 1: 0.9516441005802708\n",
      "Accuracy for Fold 2: 0.9439071566731141\n",
      "Accuracy for Fold 3: 0.9497098646034816\n",
      "Accuracy for Fold 4: 0.9516441005802708\n",
      "Accuracy for Fold 5: 0.9516441005802708\n",
      "Accuracy for Fold 6: 0.9342359767891683\n",
      "Accuracy for Fold 7: 0.9671179883945842\n",
      "Accuracy for Fold 8: 0.9458413926499033\n",
      "Accuracy for Fold 9: 0.941972920696325\n",
      "---Average Accuracy: 0.9483558994197292---\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "spam\t0.998\t\t0.85\t\t0.918\n",
      "ham\t0.928\t\t0.999\t\t0.962\n",
      "\n",
      "\n",
      "     |         s |\n",
      "     |    h    p |\n",
      "     |    a    a |\n",
      "     |    m    m |\n",
      "-----+-----------+\n",
      " ham |<3406> 264 |\n",
      "spam |    3<1497>|\n",
      "-----+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "     |             s |\n",
      "     |      h      p |\n",
      "     |      a      a |\n",
      "     |      m      m |\n",
      "-----+---------------+\n",
      " ham | <65.9%>  5.1% |\n",
      "spam |   0.1% <29.0%>|\n",
      "-----+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Provide a Baseline from emaildocs\n",
    "wordFEAT = gwFEAT(emaildocs, 2000)\n",
    "bowFEAT = [(gbREP(email, wordFEAT), label) for (email, label) in emaildocs]\n",
    "CVevalMET(10, bowFEAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Results for vocab size 100=====\n",
      "Accuracy for Fold 0: 0.8607350096711799\n",
      "Accuracy for Fold 1: 0.8994197292069632\n",
      "Accuracy for Fold 2: 0.8762088974854932\n",
      "Accuracy for Fold 3: 0.8704061895551257\n",
      "Accuracy for Fold 4: 0.8800773694390716\n",
      "---Average Accuracy: 0.8773694390715667---\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "spam\t0.949\t\t0.728\t\t0.824\n",
      "ham\t0.847\t\t0.974\t\t0.906\n",
      "\n",
      "\n",
      "     |         s |\n",
      "     |    h    p |\n",
      "     |    a    a |\n",
      "     |    m    m |\n",
      "-----+-----------+\n",
      " ham |<1528> 277 |\n",
      "spam |   40 <740>|\n",
      "-----+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "     |             s |\n",
      "     |      h      p |\n",
      "     |      a      a |\n",
      "     |      m      m |\n",
      "-----+---------------+\n",
      " ham | <59.1%> 10.7% |\n",
      "spam |   1.5% <28.6%>|\n",
      "-----+---------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "=====Results for vocab size 500=====\n",
      "Accuracy for Fold 0: 0.9032882011605415\n",
      "Accuracy for Fold 1: 0.9129593810444874\n",
      "Accuracy for Fold 2: 0.8994197292069632\n",
      "Accuracy for Fold 3: 0.9284332688588007\n",
      "Accuracy for Fold 4: 0.9284332688588007\n",
      "---Average Accuracy: 0.9145067698259187---\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "spam\t0.994\t\t0.782\t\t0.875\n",
      "ham\t0.88\t\t0.997\t\t0.935\n",
      "\n",
      "\n",
      "     |         s |\n",
      "     |    h    p |\n",
      "     |    a    a |\n",
      "     |    m    m |\n",
      "-----+-----------+\n",
      " ham |<1589> 216 |\n",
      "spam |    5 <775>|\n",
      "-----+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "     |             s |\n",
      "     |      h      p |\n",
      "     |      a      a |\n",
      "     |      m      m |\n",
      "-----+---------------+\n",
      " ham | <61.5%>  8.4% |\n",
      "spam |   0.2% <30.0%>|\n",
      "-----+---------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "=====Results for vocab size 2000=====\n",
      "Accuracy for Fold 0: 0.9458413926499033\n",
      "Accuracy for Fold 1: 0.9516441005802708\n",
      "Accuracy for Fold 2: 0.9439071566731141\n",
      "Accuracy for Fold 3: 0.9497098646034816\n",
      "Accuracy for Fold 4: 0.9516441005802708\n",
      "---Average Accuracy: 0.9485493230174081---\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "spam\t0.996\t\t0.857\t\t0.921\n",
      "ham\t0.928\t\t0.998\t\t0.962\n",
      "\n",
      "\n",
      "     |         s |\n",
      "     |    h    p |\n",
      "     |    a    a |\n",
      "     |    m    m |\n",
      "-----+-----------+\n",
      " ham |<1675> 130 |\n",
      "spam |    3 <777>|\n",
      "-----+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "     |             s |\n",
      "     |      h      p |\n",
      "     |      a      a |\n",
      "     |      m      m |\n",
      "-----+---------------+\n",
      " ham | <64.8%>  5.0% |\n",
      "spam |   0.1% <30.1%>|\n",
      "-----+---------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "=====Results for vocab size 5000=====\n",
      "Accuracy for Fold 0: 0.9593810444874274\n",
      "Accuracy for Fold 1: 0.9690522243713733\n",
      "Accuracy for Fold 2: 0.9555125725338491\n",
      "Accuracy for Fold 3: 0.9516441005802708\n",
      "Accuracy for Fold 4: 0.9497098646034816\n",
      "---Average Accuracy: 0.9570599613152805---\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "spam\t0.967\t\t0.899\t\t0.931\n",
      "ham\t0.953\t\t0.985\t\t0.969\n",
      "\n",
      "\n",
      "     |         s |\n",
      "     |    h    p |\n",
      "     |    a    a |\n",
      "     |    m    m |\n",
      "-----+-----------+\n",
      " ham |<1720>  85 |\n",
      "spam |   26 <754>|\n",
      "-----+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "     |             s |\n",
      "     |      h      p |\n",
      "     |      a      a |\n",
      "     |      m      m |\n",
      "-----+---------------+\n",
      " ham | <66.5%>  3.3% |\n",
      "spam |   1.0% <29.2%>|\n",
      "-----+---------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "=====Results for vocab size 7000=====\n",
      "Accuracy for Fold 0: 0.9555125725338491\n",
      "Accuracy for Fold 1: 0.9613152804642167\n",
      "Accuracy for Fold 2: 0.9477756286266924\n",
      "Accuracy for Fold 3: 0.9400386847195358\n",
      "Accuracy for Fold 4: 0.9535783365570599\n",
      "---Average Accuracy: 0.9516441005802708---\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "spam\t0.947\t\t0.898\t\t0.922\n",
      "ham\t0.953\t\t0.977\t\t0.965\n",
      "\n",
      "\n",
      "     |         s |\n",
      "     |    h    p |\n",
      "     |    a    a |\n",
      "     |    m    m |\n",
      "-----+-----------+\n",
      " ham |<1721>  84 |\n",
      "spam |   41 <739>|\n",
      "-----+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "     |             s |\n",
      "     |      h      p |\n",
      "     |      a      a |\n",
      "     |      m      m |\n",
      "-----+---------------+\n",
      " ham | <66.6%>  3.2% |\n",
      "spam |   1.6% <28.6%>|\n",
      "-----+---------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "=====Results for vocab size 10000=====\n",
      "Accuracy for Fold 0: 0.9439071566731141\n",
      "Accuracy for Fold 1: 0.9574468085106383\n",
      "Accuracy for Fold 2: 0.9381044487427466\n",
      "Accuracy for Fold 3: 0.9342359767891683\n",
      "Accuracy for Fold 4: 0.9400386847195358\n",
      "---Average Accuracy: 0.9427466150870406---\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "spam\t0.906\t\t0.904\t\t0.905\n",
      "ham\t0.958\t\t0.96\t\t0.959\n",
      "\n",
      "\n",
      "     |         s |\n",
      "     |    h    p |\n",
      "     |    a    a |\n",
      "     |    m    m |\n",
      "-----+-----------+\n",
      " ham |<1730>  75 |\n",
      "spam |   73 <707>|\n",
      "-----+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "     |             s |\n",
      "     |      h      p |\n",
      "     |      a      a |\n",
      "     |      m      m |\n",
      "-----+---------------+\n",
      " ham | <66.9%>  2.9% |\n",
      "spam |   2.8% <27.4%>|\n",
      "-----+---------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "=====Results for vocab size 15000=====\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-001372567154>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n=====Results for vocab size {}=====\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mword_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_word_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memaildocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mbow_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_bow_representation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memail\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0memail\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0memaildocs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mget_full_cross_validation_eval_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbow_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-73-001372567154>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n=====Results for vocab size {}=====\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mword_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_word_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memaildocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mbow_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_bow_representation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memail\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0memail\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0memaildocs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mget_full_cross_validation_eval_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbow_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-69-0564c13893c8>\u001b[0m in \u001b[0;36mget_bow_representation\u001b[1;34m(email, word_features, use_bool)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0muse_bool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m             \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memail_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0memail_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for n in [100, 500, 2000, 5000, 7000, 10000,15000]:\n",
    "    print(\"\\n=====Results for vocab size {}=====\".format(n))\n",
    "    wordFEAT = gwFEAT(emaildocs, n)\n",
    "    bowFEAT = [(gbREP(email, wordFEAT), label) for (email, label) in emaildocs]\n",
    "    CVevalMET(5, bowFEAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "BigramMeas = co_loc.BigramAssocMeasures()\n",
    "emailTOKENS = [token for email in emaildocs for token in email[0]]\n",
    "emailbigramFINDER = co_loc.BigramCollocationFinder.from_words(emailTOKENS)\n",
    "emailbigramFREQ = emailbigramFINDER.score_ngrams(BigramMeas.raw_freq)\n",
    "emailbigramPMI = emailbigramFINDER.score_ngrams(BigramMeas.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram Bag of Words\n",
    "def getbigramREP(email, bigramFEAT, use_bool=True):\n",
    "    emailbigram = [\" \".join(bigram) for bigram in ngrams(email, 2)]\n",
    "    features = {}\n",
    "    for bigram in bigramFEAT:\n",
    "        if use_bool:\n",
    "            features[bigram] = (bigram in set(emailbigram))\n",
    "        else:\n",
    "            features[bigram] = emailbigram.count(bigram)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Results for 100 bigrams=====\n",
      "\n",
      "-----Frequency-----\n",
      "Accuracy for Fold 0: 0.9690522243713733\n",
      "Accuracy for Fold 1: 0.9864603481624759\n",
      "Accuracy for Fold 2: 0.9342359767891683\n",
      "Accuracy for Fold 3: 0.6112185686653772\n",
      "Accuracy for Fold 4: 0.690522243713733\n",
      "Accuracy for Fold 5: 0.746615087040619\n",
      "Accuracy for Fold 6: 0.7330754352030948\n",
      "Accuracy for Fold 7: 0.6827852998065764\n",
      "Accuracy for Fold 8: 0.620889748549323\n",
      "Accuracy for Fold 9: 0.6808510638297872\n",
      "---Average Accuracy: 0.7655705996131528---\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham\t0.681\t\t0.983\t\t0.805\n",
      "spam\t0.972\t\t0.555\t\t0.706\n",
      "\n",
      "\n",
      "     |         s |\n",
      "     |    h    p |\n",
      "     |    a    a |\n",
      "     |    m    m |\n",
      "-----+-----------+\n",
      " ham |<2500>1170 |\n",
      "spam |   42<1458>|\n",
      "-----+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "     |             s |\n",
      "     |      h      p |\n",
      "     |      a      a |\n",
      "     |      m      m |\n",
      "-----+---------------+\n",
      " ham | <48.4%> 22.6% |\n",
      "spam |   0.8% <28.2%>|\n",
      "-----+---------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "-----PMI-----\n",
      "Accuracy for Fold 0: 0.0038684719535783366\n",
      "Accuracy for Fold 1: 0.0\n",
      "Accuracy for Fold 2: 0.09671179883945841\n",
      "Accuracy for Fold 3: 0.9941972920696325\n",
      "Accuracy for Fold 4: 0.9941972920696325\n",
      "Accuracy for Fold 5: 1.0\n",
      "Accuracy for Fold 6: 0.9980657640232108\n",
      "Accuracy for Fold 7: 1.0\n",
      "Accuracy for Fold 8: 1.0\n",
      "Accuracy for Fold 9: 0.9980657640232108\n",
      "---Average Accuracy: 0.7085106382978723---\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham\t0.998\t\t0.71\t\t0.829\n",
      "spam\t0.001\t\t0.182\t\t0.003\n",
      "\n",
      "\n",
      "     |         s |\n",
      "     |    h    p |\n",
      "     |    a    a |\n",
      "     |    m    m |\n",
      "-----+-----------+\n",
      " ham |<3661>   9 |\n",
      "spam | 1498   <2>|\n",
      "-----+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "     |             s |\n",
      "     |      h      p |\n",
      "     |      a      a |\n",
      "     |      m      m |\n",
      "-----+---------------+\n",
      " ham | <70.8%>  0.2% |\n",
      "spam |  29.0%  <0.0%>|\n",
      "-----+---------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "=====Results for 500 bigrams=====\n",
      "\n",
      "-----Frequency-----\n",
      "Accuracy for Fold 0: 0.9903288201160542\n",
      "Accuracy for Fold 1: 0.9903288201160542\n",
      "Accuracy for Fold 2: 0.9690522243713733\n",
      "Accuracy for Fold 3: 0.7098646034816247\n",
      "Accuracy for Fold 4: 0.7620889748549323\n",
      "Accuracy for Fold 5: 0.7775628626692457\n",
      "Accuracy for Fold 6: 0.8085106382978723\n",
      "Accuracy for Fold 7: 0.7872340425531915\n",
      "Accuracy for Fold 8: 0.7176015473887815\n",
      "Accuracy for Fold 9: 0.6963249516441006\n",
      "---Average Accuracy: 0.8208897485493231---\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham\t0.751\t\t0.995\t\t0.856\n",
      "spam\t0.991\t\t0.62\t\t0.763\n",
      "\n",
      "\n",
      "     |         s |\n",
      "     |    h    p |\n",
      "     |    a    a |\n",
      "     |    m    m |\n",
      "-----+-----------+\n",
      " ham |<2757> 913 |\n",
      "spam |   13<1487>|\n",
      "-----+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "     |             s |\n",
      "     |      h      p |\n",
      "     |      a      a |\n",
      "     |      m      m |\n",
      "-----+---------------+\n",
      " ham | <53.3%> 17.7% |\n",
      "spam |   0.3% <28.8%>|\n",
      "-----+---------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "-----PMI-----\n",
      "Accuracy for Fold 0: 0.08897485493230174\n",
      "Accuracy for Fold 1: 0.027079303675048357\n",
      "Accuracy for Fold 2: 0.11411992263056092\n",
      "Accuracy for Fold 3: 0.9922630560928434\n",
      "Accuracy for Fold 4: 0.9922630560928434\n",
      "Accuracy for Fold 5: 0.9980657640232108\n",
      "Accuracy for Fold 6: 0.9961315280464217\n",
      "Accuracy for Fold 7: 0.9961315280464217\n",
      "Accuracy for Fold 8: 1.0\n",
      "Accuracy for Fold 9: 0.9941972920696325\n",
      "---Average Accuracy: 0.7199226305609284---\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham\t0.995\t\t0.719\t\t0.835\n",
      "spam\t0.046\t\t0.802\t\t0.087\n",
      "\n",
      "\n",
      "     |         s |\n",
      "     |    h    p |\n",
      "     |    a    a |\n",
      "     |    m    m |\n",
      "-----+-----------+\n",
      " ham |<3653>  17 |\n",
      "spam | 1431  <69>|\n",
      "-----+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "     |             s |\n",
      "     |      h      p |\n",
      "     |      a      a |\n",
      "     |      m      m |\n",
      "-----+---------------+\n",
      " ham | <70.7%>  0.3% |\n",
      "spam |  27.7%  <1.3%>|\n",
      "-----+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [100, 500, 2000, 5000, 7000]:\n",
    "    print(\"\\n=====Results for {} bigrams=====\".format(i))\n",
    "    emailbigramFREQLIST = [\" \".join(bigram[0]) for bigram in emailbigramFREQ[:i]]\n",
    "    emailbigramPMILIST = [\" \".join(bigram[0]) for bigram in emailbigramPMI[:i]]\n",
    "    \n",
    "    print(\"\\n-----Frequency-----\".format(n))\n",
    "    bowFEAT = [(getbigramREP(email, emailbigramFREQLIST), label) for (email, label) in emaildocs]\n",
    "    CVevalMET(5, bowFEAT)\n",
    "    print(\"\\n-----PMI-----\".format(n))\n",
    "    bowFEAT = [(getbigramREP(email, emailbigramPMILIST), label) for (email, label) in emaildocs]\n",
    "    CVevalMET(5, bowFEAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature sets for documents using basic stopwords\n",
    "Basicstop = getFeatureSets1(emaildocs, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature sets for documents using extensive stopwords\n",
    "Extstop = getFeatureSets1(emaildocs, HAMtotalstop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Fold 0: 0.8701657458563536\n",
      "Accuracy for Fold 1: 0.856353591160221\n",
      "Accuracy for Fold 2: 0.8867403314917127\n",
      "Accuracy for Fold 3: 1.0\n",
      "Accuracy for Fold 4: 0.9226519337016574\n",
      "---Average Accuracy: 0.907182320441989---\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham\t0.932\t\t0.663\t\t0.775\n",
      "spam\t0.902\t\t0.985\t\t0.942\n",
      "\n",
      "\n",
      "     |    s      |\n",
      "     |    p    h |\n",
      "     |    a    a |\n",
      "     |    m    m |\n",
      "-----+-----------+\n",
      "spam |<1353> 147 |\n",
      " ham |   21 <289>|\n",
      "-----+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "     |      s        |\n",
      "     |      p      h |\n",
      "     |      a      a |\n",
      "     |      m      m |\n",
      "-----+---------------+\n",
      "spam | <74.8%>  8.1% |\n",
      " ham |   1.2% <16.0%>|\n",
      "-----+---------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "# 5 Fold Cross Validation for extensive english stop word list\n",
    "CVevalMET(5, Basicstop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Fold 0: 0.856353591160221\n",
      "Accuracy for Fold 1: 0.856353591160221\n",
      "Accuracy for Fold 2: 0.8729281767955801\n",
      "Accuracy for Fold 3: 0.9972375690607734\n",
      "Accuracy for Fold 4: 0.9226519337016574\n",
      "---Average Accuracy: 0.9011049723756906---\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "ham\t0.935\t\t0.646\t\t0.764\n",
      "spam\t0.894\t\t0.985\t\t0.937\n",
      "\n",
      "\n",
      "     |    s      |\n",
      "     |    p    h |\n",
      "     |    a    a |\n",
      "     |    m    m |\n",
      "-----+-----------+\n",
      "spam |<1341> 159 |\n",
      " ham |   20 <290>|\n",
      "-----+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "     |      s        |\n",
      "     |      p      h |\n",
      "     |      a      a |\n",
      "     |      m      m |\n",
      "-----+---------------+\n",
      "spam | <74.1%>  8.8% |\n",
      " ham |   1.1% <16.0%>|\n",
      "-----+---------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Wall time: 1min 56s\n"
     ]
    }
   ],
   "source": [
    "# 5 Fold Cross Validation for extensive stop word list\n",
    "CVevalMET(5, Extstop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
